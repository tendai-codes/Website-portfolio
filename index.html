<!DOCTYPE HTML>
<!--
	Dimension by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html> 
	<head>
		<title>Data Science Portfolio</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
		
		<!-- Highlight.js core CSS (choose your theme) -->
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css">

		<!-- Lightbox CSS -->
		<link href="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.4/css/lightbox.min.css" rel="stylesheet" /> 


	
		<style>
			/* Filter button styles */
			#project-filters {
				margin-bottom: 2rem;
				text-align: center;
			}
			
			.filter-button {
				background: #333;
				color: #fff;
				border: 1px solid #555;
				padding: 0.3rem 0.6rem;
				font-size: 0.85rem;
				margin: 0.25rem;
				cursor: pointer;
				border-radius: 4px;
				transition: all 0.3s ease;
				display: inline-block;
				text-align: cemter;
			}
			
			.filter-button:hover {
				background: #555;
			}
			
			.filter-button.active {
				background: #04a9f5;
				border-color: #04a9f5;
			}
			
			/* Project items - hidden by default */
			.project-item {
				display: none;
				margin-bottom: 2rem;
				padding: 1.5rem;
				border: 1px solid #333;
				border-radius: 8px;
			}
			
			.project-item.show {
				display: block;
			}
			
			/* Project content styling */
			.project-item h3 {
				margin-bottom: 1rem;
				color: #04a9f5;
			}
			
			.project-item h4 {
				margin-top: 1.5rem;
				margin-bottom: 0.5rem;
				color: #fff;
			}
			
			.project-item ul {
				margin-left: 1rem;
			}
			
			.project-item code {
				background: #222;
				padding: 0.2rem 0.4rem;
				border-radius: 3px;
				color: #04a9f5;
			}
			
			.button.small {
				display: inline-block;
				margin-top: 1rem;
				padding: 0.5rem 1rem;
				background: #04a9f5;
				color: #fff;
				text-decoration: none;
				border-radius: 4px;
				transition: background 0.3s ease;
			}
			
			.button.small:hover {
				background: #0390d4;
			}
			.github-button {
				background-color: #24292e;
				color: #ffffff;
				padding: 0.5rem 1rem;
				font-size: 0.95rem;
				text-align: center;
				border-radius: 6px;
				border: none;
				display: inline-block;
				margin-top: 1rem;
				transition: background 0.3s ease;
				text-decoration: none;
			}

			.github-button:hover {
				background-color: #444c56;
			}

			pre code {
				background-color: #f4f4f4;
				padding: 1rem;
				display: block;
				border-radius: 6px;
				overflow-x: auto;
				font-size: 0.9rem;
				line-height: 1.5;
				font-family: Consolas, Monaco, monospace;
			}

			.image-gallery {
			display: grid;
			grid-template-columns: repeat(2, 1fr);
			grid-template-rows: 1fr;
			gap: 10px;
			margin: 1em 0;
			max-width: 100%;
			overflow-x: auto;
			border-radius: 8px;
			padding: 10px;
			}

				
			.image-gallery img {
			width: 100%;
			height: auto;
			border-radius: 8px;
			box-shadow: 0 2px 6px rgba(0,0,0,0.1);
			object-fit: contain;
			}

			.image-gallery-1 {
			display: grid;
			grid-template-columns: repeat(1, 1fr); /* 1 equal columns */
			grid-template-rows: 1fr;
			gap: 10px;
			max-width: 100%;
			}

			figure {
				margin: 0;
			}

			figcaption {
				font-size: 0.85em;
				color: #555;
				padding-top: 0.25em;
				text-align: center;
			}

			

			pre {
			background: #f6f8fa;
			padding: 1em;
			overflow-x: auto;
			border-radius: 6px;
			}

			pre code {
			font-size: 0.9em;
			line-height: 1.4em;
			white-space: pre-wrap;       /* Wrap code lines */
  			word-break: break-word;      /* Break long words if needed */
			}

			/* lightbox image size 
			lightbox-gallery  .lbGallery {
			width: 60vw !important;
			height: auto !important;
			max-height: 70vh !important;
			object-fit: contain !important;
			} */

			.lb-overlay {
				position: fixed;
				top: 50%;
				left: 50%;
				transform: translate(-50%, -50%);
				width: 60%;
				height: auto;
				background-color: white;
				border-radius: 10px;
				box-shadow: 0 0 20px rgba(0,0,0,0.5);
			}

			lightbox-gallery .lbImage {
			max-width: 60vh !important;
			height: auto !important;
			margin: 0 auto;
			}

			

		</style>	
	
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<div class="logo">
							<span class="icon fa-gem"></span>
						</div>
						<div class="content">
							<div class="inner">
								<h1>Tendai N Sibanda <br> Data Science Portfolio</h1>
								<p>Making sense of complexity through data. Leveraging AI to empower people and decisions.</p>
							</div>
						</div>
						<nav>
							<ul>
								<li><a href="#intro">Intro</a></li>
								<li><a href="#projects">Projects</a></li>
								<li><a href="#about">About Me</a></li>
								<li><a href="#contact">Contact</a></li>
								<!--<li><a href="#elements">Elements</a></li>-->
							</ul>
						</nav>
					</header>

				<!-- Main -->
					<div id="main">

						<!-- Intro -->
							<article id="intro">
								<h2 class="major">Intro</h2>
								<span class="image main"><img src="images/roadmap.png" alt="" /></span>
								<p>This portfolio captures my journey from data analytics to machine learning. I began with projects in R and soon transitioned to Python, where I discovered the world of AI. Along the way, I‚Äôve learned to build models catering to supervised and unsupervised learning. I‚Äôm currently getting more familiar with deep learning‚Äîspecifically computer vision and retrieval-augmented generation. Check out some of my work <a href="#projects"> here</a>.</p>
								
							</article>

						<!-- Projects -->
							<article id="projects">
								<h2 class="major">Projects</h2>
								
								<span class="image main"><img src="images/datascience.png" alt="" /></span>
								<p>This section showcases hands-on projects where I apply Python and R to solve real-world problems. From data cleaning to model evaluation, each project includes a clear data pipeline, code snippets, and key takeaways. You‚Äôll also find notes on the challenges I faced and how I overcame them, offering insight into my problem-solving approach.</p>
								
							
								<h2 class="major">Project Types</h2>

								<div id="project-filters">
									
									<button data-tag="R" class="filter-button">R</button>
									<button data-tag="Python" class="filter-button">Python</button>
									<button data-tag="Data Analysis" class="filter-button">Data Analysis</button>
									<button data-tag="Machine Learning" class="filter-button">Machine Learning</button>
									<button data-tag="PyTorch" class="filter-button">PyTorch</button>
									<button data-tag="TensorFlow" class="filter-button">TensorFlow</button>
									<button data-tag="Deep Learning" class="filter-button">Deep Learning</button>
							
									<button data-tag="all" class="filter-button active">All</button>
									
                				</div>

								<div id="projects-container">

									<section class="project-item" data-tags="R,Data Analysis">
										<h3>üìΩÔ∏è <strong>Box Office Data Analysis </strong></h3>
										<p>
											This project explored publicly available data on top-grossing US films to identify patterns across genres, studios, and release schedules. I used <strong>R</strong> and <strong>ggplot2</strong> for data exploration and visualisation.
										</p>

										<h4>üíª <strong>Tech Stack:</strong></h4>
										<ul>
											<li><strong>R</strong> for data manipulation and visualisation</li>
											<li><strong>ggplot2</strong> for creating custom, layered visual insights</li>
										</ul>

										<h4>üß™ <strong>Data Pipeline:</strong></h4>
										<ul>
											<li><strong>Import & inspect data:</strong> Used <code>read.csv()</code>, <code>summary()</code>, and <code>str()</code></li>
											<li><strong>Initial exploration:</strong> Identified no Monday releases using a bar plot of <code>Day.of.Week</code></li>
											<li><strong>Filtering for significance:</strong> Narrowed to key genres and major studios</li>
											<li><strong>Visualisation:</strong> Created jitter + box plots comparing domestic gross</li>
											<li><strong>Aesthetics:</strong> Tuned themes for clarity and presentation</li>
										</ul>

										<h4>üìä <strong>Code Snippets & Visualisations:</strong></h4>

										<!-- Code Snippet -->

										<pre><code class="language-r"># No movies are ever released on a Monday. (Figure 1)
										ggplot(data=mov, aes(x=Day.of.Week)) + geom_bar()

										# Filter dataset for desired genres:
										filt <- (mov$Genre == "action") | (mov$Genre == "adventure") | 
												(mov$Genre == "animation") | (mov$Genre == "comedy") | (mov$Genre == "drama")

										# Filter dataset for desired studios:
										filt2 <- (mov$Studio == "Buena Vista Studios") | (mov$Studio == "WB") | 
												(mov$Studio == "Fox") | (mov$Studio == "Universal") | 
												(mov$Studio == "Sony") | (mov$Studio == "Paramount Pictures")

										# Apply filters
										mov2 <- mov[filt & filt2,]

										# Prepare the plot's data and aes layers (Figure 2)
										p <- ggplot(data=mov2, aes(x=Genre, y=Gross...US))

										q <- p +
										geom_jitter(aes(size=Budget...mill., colour=Studio)) +
										geom_boxplot(alpha = 0.7, outlier.colour = NA)

										# Non-data info
										q +
										xlab("Genre") + 
										ylab("Gross % US") + 
										ggtitle("Domestic Gross % by Genre")

										# Theme
										q <- q +
										theme(
											text = element_text(family="Times New Roman"),
											axis.title.x = element_text(colour="Blue", size=30),
											axis.title.y = element_text(colour="Blue", size=30),
											axis.text.x = element_text(size=20),
											axis.text.y = element_text(size=20),
											plot.title = element_text(colour="Black", size=40),
											legend.title = element_text(size=20),
											legend.text = element_text(size=12)
										)
										</code></pre>

										
										<!-- Visualisations -->

										<div class="image-gallery">
										<figure>
											<a href="images/box_office.png" data-lightbox="box-analysis" data-title="Figure 1: No releases on Mondays">
											<img src="images/box_office.png" alt="Figure 1: No releases on Mondays">
											</a>
											<figcaption><strong>Figure 1</strong> No releases on Mondays</figcaption>
										</figure>

										<figure>
											<a href="images/box_genre.png" data-lightbox="box-analysis" data-title="Figure 2: Domestic gross by genre">
											<img src="images/box_genre.png" alt="Figure 2: Domestic gross by genre">
											</a>
											<figcaption><strong>Figure 2</strong> Domestic gross by genre</figcaption>
										</figure>
										</div>

										<h4>üåü <strong>Key Insights:</strong></h4>
										<p>
											Profitable genres are concentrated among a few studios. Monday releases are avoided ‚Äî possibly a scheduling strategy.
										</p>

										<h4>üßóüèæ <strong>Challenge Faced:</strong></h4>
										<p>
											Overlapping outliers and jitter points in ggplot2 caused clutter. I resolved this with <code>outlier.colour = NA</code> and <code>alpha</code> blending.
										</p>
										<p><a href="https://github.com/tendai-codes/your-project" class="github-button" target="_blank">View on GitHub</a></p>
										</p>
									</section>

									<section class="project-item" data-tags="R,Data Analysis">
										<h3>üìΩÔ∏è <strong>Financial Statement Analysis </strong></h3>
										<p>
											This project analysed five years of company financial data (2018‚Äì2022), including Balance Sheet, Income Statement, and Cash Flow Statement. I used Python and pandas to calculate key financial ratios, identify operational trends, and visualise multi-year performance.
										</p>

										<h4>üíª <strong>Tech Stack:</strong></h4>
										<ul>
											<li><strong>R</strong> for data manipulation and analysis</li>
											<li><strong>Pandas</strong> for computations</li>
											<li><strong>Matplotlib</strong> for plotting financial visualisations</li>
										</ul>

										<h4>üß™ <strong>Data Pipeline:</strong></h4>
										<ul>
											<li><strong>Load & inspect data:</strong> Imported multi-statement CSV into pandas. Extracted relevant sections for Balance Sheet, Income Statement, and Cash Flow Statement using <code>loc[]</code>. </li>
											<li><strong>Trend Analysis:</strong> Plotted major components (e.g. assets, equity) using <code>.plot()</code> to visualise financial stability over time. </li>
											<li><strong>Ratio Calculations:</strong> Computed solvency and profitability ratios: Return on Equity (ROE), Return on Assets (ROA), Debt-to-Equity. Built <code>DataFrame</code> to summarise and visualise using grouped bar plots.</li>
											<li><strong>Custom Metrics:</strong> Created Operating Cash Flow to Total Debt ratio from cash flow and balance sheet sections to assess short-term liquidity strength.</li>
											
										</ul>

										<h4>üìä <strong>Code Snippets & Visualisations:</strong></h4>

										<!-- Code Snippet -->

										<pre><code class="language-r"># Load required library for visualizations
										library(ggplot2)
										library(tidyverse)
										# Data
										revenue <- c(14574.49, 7606.46, 8611.41, 9175.41, 8058.65, 8105.44, 11496.28, 9766.09, 10305.32, 14379.96, 10713.97, 15433.50)
										expenses <- c(12051.82, 5695.07, 12319.20, 12089.72, 8658.57, 840.20, 3285.73, 5821.12, 6976.93, 16618.61, 10054.37, 3803.96)

										# Calculate Profit As The Differences Between Revenue And Expenses
										profit <- revenue - expenses
										profit

										# Calculate Tax As 30% Of Profit And Round To 2 Decimal Points
										tax <- round(0.30 * profit, 2)
										tax

										# Calculate Profit Remaining After Tax Is Deducted
										profit.after.tax <- profit - tax
										profit.after.tax

										#Visualize Profit after tax
										# Create a data frame for visualization
										data <- data.frame(
										Month = 1:12,
										Profit_After_Tax = profit.after.tax)

										# Create the bar chart using ggplot2 (Figure 1)
										ggplot(data, aes(x = factor(Month), y = Profit_After_Tax)) +
										geom_bar(stat = "identity", fill = "steelblue") +
										labs(title = "Monthly Profit After Tax", x = "Month", y = "Profit After Tax") +
										theme_minimal()

										# Calculate The Profit Margin As Profit After Tax Over Revenue
										profit.margin <- round(profit.after.tax / revenue, 2) * 100
										profit.margin

										# Visualize Profit Margin
										# Create a data frame for visualization
										data <- data.frame(
										Month = 1:12,
										Profit_margin = profit.margin)

										# Create the bar chart using ggplot2 (Figure 2)
										ggplot(data, aes(x = factor(Month), y = profit.margin)) +
										geom_bar(stat = "identity", fill = "orange") +
										labs(title = "Monthly Profit Margin (%)", x = "Month", y = "Profit Margin (%)") +
										theme_minimal()

										# Calculate The Mean Profit After Tax For The 12 Months
										mean_pat <- mean(profit.after.tax)
										mean_pat

										# Find The Months With Above-Mean Profit After Tax
										good.months <- profit.after.tax > mean_pat
										good.months

										# Bad Months Are The Opposite Of Good Months
										bad.months <- !good.months
										bad.months

										# The Best Month Is The Month With The Highest Profit After Tax
										best.month <- profit.after.tax == max(profit.after.tax)
										best.month

										# The Worst Month Is The Month With The Lowest Profit After Tax
										worst.month <- profit.after.tax == min(profit.after.tax)
										worst.month
										</code></pre>

										
										<!-- Visualisations -->

										<div class="image-gallery">
										<figure>
											<a href="images/Financial_statement_b.png" data-lightbox="finance-state" data-title="Figure 1: Profit After Tax">
											<img src="images/Financial_statement_b.png" alt="Figure 1: Profit After Tax">
											</a>
											<figcaption><strong>Figure 1</strong> Profit After Tax</figcaption>
										</figure>

										<figure>
											<a href="images/Financial_statement_O.png" data-lightbox="finance-state" data-title="Figure 2: Profit Margin">
											<img src="images/Financial_statement_O.png" alt="Figure 2: Profit Margin">
											</a>
											<figcaption><strong>Figure 2</strong> Profit Margin</figcaption>
										</figure>
										</div>

										<h4>üåü <strong>Key Insights:</strong></h4>
										<p>
											The company showed stable asset growth and rising equity, but the Debt-to-Equity ratio increased post-2020, signalling higher leverage risk.
											Operating cash flow was consistently positive, suggesting sufficient liquidity to meet short-term obligations ‚Äî a green flag for operational health.
										</p>

										<h4>üßóüèæ <strong>Challenge Faced:</strong></h4>
										<p>
											Initially, aligning the financial statements by year was inconsistent due to mixed string/index formats across categories. I overcame this by explicitly extracting year-based columns and standardising label references. This made ratio computations and cross-statement comparisons reliable and reproducible.
										</p>
										<p><a href="https://github.com/tendai-codes/your-project" class="github-button" target="_blank">View on GitHub</a></p>
									</section>

									<section class="project-item" data-tags="Python,Data Analysis">
										<h3>üìΩÔ∏è <strong>Global Population Trends Exploration </strong></h3>
										<p>
											This project used World Bank development indicators to explore trends in population growth, urbanisation, and fertility rates across continents and income groups. The objective was to uncover insights about global development patterns over time using Python.
										</p>

										<h4>üíª <strong>Tech Stack:</strong></h4>
										<ul>
											<li><strong>Python</strong> for data handling and exploration</li>
											<li><strong>Pandas</strong> for data manipulation</li>
											<li><strong>Matplotlib</strong> for visualisations</li>
										</ul>

										<h4>üß™ <strong>Data Pipeline:</strong></h4>
										<ul>
											<li><strong>Load & inspect data:</strong> Loaded the dataset using <code>pd.read_csv()</code> and inspected structure with <code>.info()</code> and <code>.head()</code> to understand column types and missing data. </li>
											<li><strong>Cleaning:</strong> Renamed columns, removed irrelevant rows, and addressed missing values for smoother analysis. </li>
											<li><strong>Initial Exploaration:</strong> Examined fertility rates, population growth, and urban population across income levels and continents.</li>
											<li><strong>Grouping & Summarisation:</strong> Used <code>groupby()</code> and <code>mean()</code> to aggregate indicators by continent and income level.</li>
											<li><strong>Visualisation:</strong> Created scatter plots, line plots, and box plots to reveal relationships between population metrics and economic status.</li>
											
										</ul>

										
										<h4>üìä <strong>Code Snippets & Visualisations:</strong></h4>

										<!-- Code Snippet -->

										<pre><code class="language-python"># Plot the BirthRate versus Internet Users categorised by Income Group (Figure 1)
										vis1 = sns.lmplot( data = data, x = 'BirthRate', y = 'InternetUsers', fit_reg = False, hue = 'IncomeGroup', height = 10 )

										# Create the dataframe
										country_data = pd.DataFrame({'CountryName': np.array(Countries_2012_Dataset),
																	'CountryCode': np.array(Codes_2012_Dataset),
																	'CountryRegion': np.array(Regions_2012_Dataset)})

										# Merge country data to the original dataframe (Table 1)
										merged_data = pd.merge(left=data, right=country_data, how='inner', on="CountryCode")
										merged_data.head()

										# Create a data frame with the life expectancy
										life_exp_data = pd.DataFrame({'CountryCode': np.array(Country_Code),
																	'LifeExp1960': np.array(Life_Expectancy_At_Birth_1960),
																	'LifeExp2013': np.array(Life_Expectancy_At_Birth_2013)})

										# Merge the data frame with the life expectancy
										merged_data1 = pd.merge(left=merged_data, right=life_exp_data, how='inner', on='CountryCode')

										# Explore the dataset (Table 2)
										merged_data1.head()

										# Plot the BirthRate versus LifeExpectancy cathegorised by Country Region in 1960 (Figure 3)
										vis3 = sns.lmplot( data = merged_data1, x = 'BirthRate', y = 'LifeExp1960', fit_reg = False, hue = 'CountryRegion', height = 10 )
										Plot

										# Plot the BirthRate versus LifeExpectancy cathegorised by Country Region in 2013 (Figure 4)
										vis4 = sns.lmplot( data = merged_data1, x = 'BirthRate', y = 'LifeExp2013', fit_reg = False, hue = 'CountryRegion', height = 10 )										
										
									
										</code></pre>

										<div class="image-gallery">
										<figure>
											<a href="images/world_dfmerge.png" data-lightbox="world-bank" data-title="Table 1: Merged DF">
											<img src="images/world_dfmerge.png" alt="Table 1: Merged DF">
											</a>
											<figcaption><strong>Table 1</strong></figcaption>
										</figure>

										<figure>
											<a href="images/world_dfmerge2.png" data-lightbox="world-bank" data-title="Table 2: Merged DF 2">
											<img src="images/world_dfmerge2.png" alt="Table 2: Merged DF 2">
											</a>
											<figcaption><strong>Table 2</strong></figcaption>
										</figure>

										<figure>
											<a href="images/world_birthintIG.png" data-lightbox="world-bank" data-title="Figure 1: birthintIG">
											<img src="images/world_birthintIG.png" alt="Figure 1: birthintIG">
											</a>
											<figcaption><strong>Figure 1</strong> BirthRate versus Internet Users categorised by Income Group</figcaption>
										</figure>

										<figure>
											<a href="images/world_birthintCR.png" data-lightbox="world-bank" data-title="Figure 2: birthintCR">
											<img src="images/world_birthintCR.png" alt="Figure 2: birthintCR">
											</a>
											<figcaption><strong>Figure 2</strong> BirthRate versus Internet Users categorised by Country Region</figcaption>
										</figure><figure>
											<a href="images/world_life1960.png" data-lightbox="world-bank" data-title="Figure 3: world_life1960">
											<img src="images/world_life1960.png" alt="Figure 3: world_life1960">
											</a>
											<figcaption><strong>Figure 3</strong> BirthRate versus LifeExpectancy in 1960</figcaption>
										</figure>

										<figure>
											<a href="images/world_life2013.png" data-lightbox="world-bank" data-title="Figure 4: Merged DF 2">
											<img src="images/world_life2013.png" alt="Figure 4: world_life2013">
											</a>
											<figcaption><strong>Figure4</strong> BirthRate versus LifeExpectancy in 2013</figcaption>
										</figure>
										</div>

										

										<h4>üåü <strong>Key Insights:</strong></h4>
										<ul>
											<li>Countries with lower income levels showed higher fertility rates and population growth.</li>
											<li>Urban population tends to correlate with income level, especially in developed regions.</li>
											<li>Africa stands out with higher fertility rates and population growth compared to other continents.</li>
										
										</ul>

										<h4>üßóüèæ <strong>Challenge Faced:</strong></h4>
										<p>
											Filtering and reshaping the dataset for multi-variable analysis was complex due to inconsistent column names and missing data. I solved this by methodically renaming columns and using <code>.dropna()</code> to exclude incomplete records while maintaining dataset integrity.
										</p>
										<p><a href="https://github.com/tendai-codes/your-project" class="github-button" target="_blank">View on GitHub</a></p>
									</section>


									<section class="project-item" data-tags="Python,Data Analysis">
										<h3>üìΩÔ∏è <strong> Stock Market Analysis </strong></h3>
										<p>
											This project analyzed historical stock price data for major companies and the S&P 500 index to understand price movements, correlations, and daily return patterns. I built an interactive dashboard using Python's data science stack to visualize both raw and normalized stock performance alongside risk metrics.
										</p>

										<h4>üíª <strong>Tech Stack:</strong></h4>
										<ul>
											<li><strong>Python</strong> for data manipulation and analysis</li>
											<li><strong>Pandas</strong> for computations</li>
											<li><strong>Matplotlib & Seaborn</strong> for static visualisations</li>
											<li><strong>Plotly</strong> for interactive charts and dashboards</li>
											<li><strong>NumPy & SciPy</strong> for statistical computations</li>
										</ul>

										<h4>üß™ <strong>Data Pipeline:</strong></h4>
										<ul>
											<li><strong>Explore data:</strong> Loaded stock price data using <code>pd.read_csv()</code> and explored the dataset structure with <code>.info()</code>, <code>.describe()</code>, and <code>.head()</code> to understand the time series format and identify key stocks. Checked for missing values using <code>.isnull().sum()</code> and calculated basic statistics like mean returns and standard deviation to assess data completeness and variability.</li>
											<li><strong>Price Normalisation:</strong> Created a custom <code>normalize()</code> function to standardize all stock prices to their starting values, enabling fair comparison of relative performance across different price ranges. </li>
											<li><strong>Daily Returns Calculations:</strong> Built a <code>daily_return()</code> function using nested loops to compute percentage daily returns: <code>((current_price - previous_price) / previous_price) * 100</code> for each stock.</li>
											<li><strong>Visualisation:</strong> Developed reusable plotting functions <code>show_plot()</code> and <code>interactive_plot()</code> to create both static matplotlib charts and interactive Plotly visualizations for raw prices, normalized prices, and daily returns.</li>
											<li><strong>Correlation Analysis:</strong> Generated a correlation matrix using <code>.corr()</code> and visualized it with a Seaborn heatmap to identify relationships between stock movements.</li>
											<li><strong>Distribution Analysis:</strong> Created histograms and compiled distribution plots using Plotly's <code>create_distplot()</code> to analyze the statistical properties of daily returns.</li>
										</ul>

										<h4>üìä <strong>Code Snippets & Visualisations:</strong></h4>

										<!-- Code Snippet -->

										<pre><code class="language-python"># Visualize Stocks data
										def show_plot(df, title):
										df.plot(x = 'Date', figsize=(12, 8), linewidth = 3, title=title)
										plt.xlabel('Date')
										plt.ylabel('Price')
										plt.grid()
										plt.show()

										# Plot the data (Figure 1)
										show_plot(stocks_df, 'STOCKS DATA')
										
										# Normalized Stock Data (Figure 2)
										def normalize(df):
											x = df.copy()
											for i in x.columns[1:]:
												x[i] = x[i]/x[i][0]
											return x

										normalize(stocks_df)

										# Create Interactive chart of Stock Data (Figure 3)

										def interactive_plot(df, title):
										fig = px.line(title=title)

										for i in df.columns[1:]:
											fig.add_scatter(x=df['Date'], y=df[i], name=i)
											fig.update_layout(
											xaxis_title = "Date",
											yaxis_title = "Price"
										)
										fig.show()
										interactive_plot(stocks_df, 'STOCKS DATA')
										
										# Create Interactive chart of Normalized Stock Data (Figure 4)
										interactive_plot(normalize(stocks_df), 'STOCKS DATA')

										#Calculate stocks daily returns

										def daily_return(df):
										df_daily_return = df.copy()
										#loop for columns
										for i in df.columns[1:]:
										#loop for rows
											for j in range(1, len(df)):
											df_daily_return[i][j] = ((df[i][j] - df[i][j-1]) / (df[i][j-1]))*100

											df_daily_return[i][0] = 0

										return df_daily_return


										# Get the daily returns (Figure 5)
										stocks_daily_return = daily_return(stocks_df)
										stocks_daily_return

										interactive_plot(stocks_daily_return, 'Stocks Daily returns')


										# Daily Return Correlation
										cm = stocks_daily_return.drop(columns = ['Date']).corr()
										cm
										# heatmap showing correlations (Figure 6)
										plt.figure(figsize=(10, 8))
										sns.heatmap(cm, annot=True, cmap='RdYlGn') #true give you the values on the heatmap
										plt.show()

										# Histogram of daily returns (Figure 7)
										stocks_daily_return.hist(bins=50, figsize=(20, 10))
										plt.show();

										</code></pre>

										
										<!-- Visualisations -->

										<div class="image-gallery">
										<figure>
											<a href="images/stocks_1.png" data-lightbox="stock-data" data-title="Figure 1: stocks_1">
											<img src="images/stocks_1.png" alt="Figure 1: stocks_1">
											</a>
											<figcaption><strong>Figure 1</strong></figcaption>
										</figure>

										<figure>
											<a href="images/stocknormal1.png" data-lightbox="stock-data" data-title="Figure 2: stocknormal1">
											<img src="images/stocknormal1.png" alt="Figure 2: stocknormal1">
											</a>
											<figcaption><strong>Figure 2</strong></figcaption>
										</figure>

										<figure>
											<a href="images/stocksdata.png" data-lightbox="stock-data" data-title="Figure 3: stocksdata">
											<img src="images/stocksdata.png" alt="Figure 3: stocksdata">
											</a>
											<figcaption><strong>Figure 3</strong></figcaption>
										</figure>

										<figure>
											<a href="images/stockdatan.png" data-lightbox="stock-data" data-title="Figure 4: stocks data normal">
											<img src="images/stockdatan.png" alt="Figure 4: stocks data normal">
											</a>
											<figcaption><strong>Figure 4</strong></figcaption>
										</figure>
										
										<figure>
											<a href="images/stockdailyr.png" data-lightbox="stock-data" data-title="Figure 5: stockdailyr">
											<img src="images/stockdailyr.png" alt="Figure 5: stockdailyr">
											</a>
											<figcaption><strong>Figure 5</strong></figcaption>
										</figure>

										<figure>
											<a href="images/stockdataheat.png" data-lightbox="stock-data" data-title="Figure 6: stockdataheat">
											<img src="images/stockdataheat.png" alt="Figure 2: stockdataheat">
											</a>
											<figcaption><strong>Figure 6</strong></figcaption>
										</figure>
										
										<figure>
											<a href="images/stockdatahist.png" data-lightbox="stock-data" data-title="Figure 7: stockdatahist">
											<img src="images/stockdatahist.png" alt="Figure 2: stockdatahist">
											</a>
											<figcaption><strong>Figure 7</strong></figcaption>
										</figure>
										</div>



										<h4>üåü <strong>Key Insights:</strong></h4>
										<ul>
											<li>Amazon (AMZN) showed the highest maximum price during the 2012-2020 period</li>
											<li>The S&P 500 provided a solid benchmark for market performance comparison</li>
											<li>Stock correlations revealed clusters of related companies that move together</li>
											<li>Daily return distributions showed varying levels of volatility across different stocks</li>
										</ul>


										<h4>üßóüèæ <strong>Challenge Faced:</strong></h4>
										<p>
											The daily returns calculation initially produced incorrect values for the first row of each stock. After debugging, the issue was that there's no previous day to calculate a return from for the first entry. This was solved by explicitly setting the first day's return to 0 using <code>df_daily_return[i][0] = 0</code> after the loop calculation, ensuring accurate percentage calculations for all subsequent days.
										</p>
										<p><a href="https://github.com/tendai-codes/your-project" class="github-button" target="_blank">View on GitHub</a></p>
									</section>

									<section class="project-item" data-tags="Python,Data Analysis">
										<h3>üìΩÔ∏è <strong>Portfolio Optimization & Risk Analysis </strong></h3>
										<p>
											This project built a comprehensive portfolio management system that simulates random asset allocation across major stocks and calculates key financial metrics including returns, volatility, and risk-adjusted performance. I developed a complete portfolio analytics framework using Python to evaluate investment strategies and portfolio performance over time.
										</p>

										<h4>üíª <strong>Tech Stack:</strong></h4>
										<ul>
											<li><strong>Python</strong> for financial calculations and portfolio modeling</li>
											<li><strong>Pandas</strong> for time series data manipulation and financial computations</li>
											<li><strong>NumPy</strong> for random weight generation and mathematical operations</li>
											<li><strong>Plotly & SciPy</strong> for interactive portfolio performance visualisation, statistical analysis and risk metrics </li>
										</ul>

										<h4>üß™ <strong>Data Pipeline:</strong></h4>
										<ul>
											<li><strong>Data Preparation:</strong> Loaded and sorted stock data chronologically using <code>sort_values()</code> by Date to ensure proper time series analysis for portfolio calculations. </li>
											<li><strong>Random Portfolio Generation:</strong> Used <code>np.random.seed()</code> and <code>np.random.seed(9)</code> to create randomized asset allocation weights, then normalized them using <code>weights / np.sum(weights)</code> to ensure they sum to 100%. </li>
											<li><strong>Portfolio Normalisation:</strong> Applied a custom normalize() function to standardize all stock prices to their initial values, creating a baseline for relative performance comparison across different price ranges.</li>
											<li><strong>Portfolio function development:</strong> Built a reusable <code>portfolio_allocation()</code> function that encapsulates the entire workflow for testing different weight combinations and portfolio strategies.</li>
											<li><strong>Risk metrics calculation:</strong> Computed cumulative return, standard deviation (volatility), average daily return, and Sharpe ratio (assessesment of the risk-adjusted returns of an investment) using <code>np.sqrt(252)</code> for annualization.</li>
										
										</ul>

										<h4>üìä <strong>Code Snippets & Visualisations:</strong></h4>

										<!-- Code Snippet -->

										<pre><code class="language-python">#Create random portfolio weights
										np.random.seed()

										# Create random weights for the stocks
										weights = np.array(np.random.random(9))

										# Random Asset Allocation & Calculate Portfolio Daily Return
										weights = weights / np.sum(weights)
										print(weights)

										# Define Normalization function
										def normalize(df):
										x = df.copy()
										for i in x.columns[1:]:
											x[i] = x[i]/x[i][0]
										return x

										# Enumerate returns the value and a counter as well
										for counter, stock in enumerate(df_portfolio.columns[1:]):
										df_portfolio[stock] = df_portfolio[stock] * weights[counter]
										df_portfolio[stock] = df_portfolio[stock] * 1000000
										df_portfolio

										# Calculate the portfolio daily return

										df_portfolio['portfolio daily % return'] = 0.0000

										for i in range(1, len(stocks_df)):
										# Calculate the percentage of change from the previous day
										df_portfolio['portfolio daily % return'][i] = ( (df_portfolio['portfolio daily worth/$'][i] - df_portfolio['portfolio daily worth/$'][i-1]) / df_portfolio['portfolio daily worth/$'][i-1]) * 100

										df_portfolio

										# Create a function for stock portfolio allocation
										# Assume $1000000 is total amount for portfolio

										def portfolio_allocation(df, weights):

										df_portfolio = df.copy()

										# Normalize the stock avalues
										df_portfolio = normalize(df_portfolio)

										for counter, stock in enumerate(df_portfolio.columns[1:]):
											df_portfolio[stock] = df_portfolio[stock] * weights[counter]
											df_portfolio[stock] = df_portfolio[stock] * 1000000

										df_portfolio['portfolio daily worth in $'] = df_portfolio[df_portfolio != 'Date'].sum(axis = 1)

										df_portfolio['portfolio daily % return'] = 0.0000

										for i in range(1, len(stocks_df)):

										# Calculate the percentage of change from the previous day
											df_portfolio['portfolio daily % return'][i] = ( (df_portfolio['portfolio daily worth in $'][i] - df_portfolio['portfolio daily worth in $'][i-1]) / df_portfolio['portfolio daily worth in $'][i-1]) * 100

										# Set the value of first row to zero, as previous value is not available
										df_portfolio['portfolio daily % return'][0] = 0
										return df_portfolio

										# Plot the portfolio daily return (Figure 1)
										fig = px.line(x = df_portfolio.Date, y = df_portfolio['portfolio daily % return'], title = 'Portfolio Daily % Return',
													labels = {
															"x": "Date",
															"y": "Daily Percentage Return"
														})
										fig.show()

										# Cummulative return of the portfolio
										cummulative_return = ((df_portfolio['portfolio daily worth/$'][-1:] - df_portfolio['portfolio daily worth/$'][0])/ df_portfolio['portfolio daily worth/$'][0]) * 100
										print('Cummulative return of the portfolio is {} %'.format(cummulative_return.values[0]))

										# Calculate the average daily return
										print('Average daily return of the portfolio is {} %'.format(df_portfolio['portfolio daily % return'].mean() ))

										# Portfolio sharpe ratio
										sharpe_ratio = df_portfolio['portfolio daily % return'].mean() / df_portfolio['portfolio daily % return'].std() * np.sqrt(252)
										print('Sharpe ratio of the portfolio is {}'.format(sharpe_ratio))

										</code></pre>

										
										<!-- Visualisations -->

										<div class="image-gallery">
										
										<figure>
											<a href="images/portfoliodaily.png" data-lightbox="portfolio-assets" data-title="Figure 1: portfoliodaily">
											<img src="images/portfoliodaily.png" alt="Figure 1: portfoliodaily">
											</a>
											<figcaption><strong>Figure 1</strong> Portfolio Daily Returns (%)</figcaption>
										</figure>
										</div>


										<h4>üåü <strong>Key Insights:</strong></h4>
										<ul>
											<li>The random portfolio allocation provided a baseline for comparing systematic investment strategies</li>
											<li>Sharpe ratio calculation revealed the risk-adjusted return efficiency of the portfolio</li>
											<li>Daily return volatility patterns showed the importance of diversification across different stocks</li>
											<li>Cumulative returns demonstrated the compound effect of daily performance over the investment period</li>
										</ul>

										<h4>üßóüèæ <strong>Challenge Faced:</strong></h4>
										<p>
											I initially encountered an indexing error when calculating portfolio daily returns because I was trying to access the previous day's value for the first row, which doesn't exist. The calculation <code>df_portfolio['portfolio daily % return'][i-1]</code> failed on the first iteration. I solved this by explicitly setting the first day's return to 0 using <code>df_portfolio['portfolio daily % return'][0] = 0</code> after the loop, and ensuring the loop started from index 1 rather than 0. This approach properly handled the edge case while maintaining accurate percentage calculations for all subsequent trading days.
										</p>
										<p><a href="https://github.com/tendai-codes/your-project" class="github-button" target="_blank">View on GitHub</a></p>
									</section>


									<section class="project-item" data-tags="Python,Data Analysis">
										<h3>üìΩÔ∏è <strong>Stock Beta & Alpha Analysis (CAPM Model) </strong></h3>
										<p>
											This project implemented the Capital Asset Pricing Model (CAPM) to analyze individual stock performance relative to the S&P 500 market benchmark. I calculated beta (market sensitivity) and alpha (excess returns) for each stock using linear regression, creating comprehensive risk-return profiles for portfolio decision-making.
										</p>

										<h4>üíª <strong>Tech Stack:</strong></h4>
										<ul>
											<li><strong>Python</strong> for financial calculations and statistical analysis</li>
											<li><strong>Pandas</strong> for time series data manipulation and daily returns calculations </li>
											<li><strong>NumPy</strong> for linear regression and polynomial fitting</li>
											<li><strong>Seaborn</strong> for enhanced statistical plotting capabilities</li>
											<li><strong>Plotly Express</strong> for interactive CAPM analysis charts</li>
											<li><strong>Matplotlib</strong> for static scatter plots and regression line visualisation</li>
										</ul>

										<h4>üß™ <strong>Data Pipeline:</strong></h4>
										<ul>
											<li><strong>Market benchmark analysis:</strong> Used S&P 500 as the market proxy and calculated average daily returns using <code>.drop('Date', axis=1).mean()</code> to establish baseline market performance. </li>
											<li><strong>Beta & Alpha computation:</strong> Applied <code>np.polyfit()</code> with order=1 to perform linear regression between individual stock returns and S&P 500 returns, extracting beta (slope) and alpha (intercept) coefficients. </li>
											<li><strong>Batch analysis automation:</strong> Developed loops to iterate through all stocks (excluding S&P 500 and Date columns) using conditional statements <code>if i != 'sp500'</code> and <code>i != 'Date'</code> to calculate beta and alpha for each stock systematically.</li>
											<li><strong>Interactive dashboard creation:</strong> Built Plotly Express scatter plots with <code>px.scatter()</code> and added regression lines using <code>fig.add_scatter()</code> to create interactive CAPM analysis charts for each stock.</li>
											<li><strong>Risk metrics storage:</strong> Used Python dictionaries <code>beta = {}</code> and <code>alpha = {}</code> to store calculated coefficients for each stock, enabling easy comparison and further analysis. </li>
										</ul>

										<h4>üìä <strong>Code Snippets & Visualisations:</strong></h4>

										<!-- Code Snippet -->

										<pre><code class="language-python"># Function to calculate the daily returns
										def daily_returns(df):
										df_daily_return = df.copy()
										for i in df.columns[1:]:
											for j in range(1, len(df)):
											df_daily_return[i][j] = ((df[i][j] - df[i][j-1])/df[i][j-1])*100

											df_daily_return[i][0] = 0
										return df_daily_return

										# Plot a scatter plot between the selected stock and the S&P500 (Market) (Figure 1)
										plt.scatter(stocks_daily_return['sp500'], stocks_daily_return['AAPL'])
										plt.xlabel('sp500')
										plt.ylabel('AAPL')
										plt.grid()

										# Add beta & alpha to plot
										beta, alpha = np.polyfit(stocks_daily_return['sp500'], stocks_daily_return['AAPL'], 1)

										# Add regression line (beta) - y = beta [stockvsmkt- stock volatility] * rm [stock daily return] + alpha [excess return on top of mkt return]
										plt.plot(stocks_daily_return['sp500'], beta * stocks_daily_return['sp500'] + alpha, '-', color = 'r')
										plt.show()

										# Let's calculate the annualized rate of return for S&P500 (Assume 252 working days per year)

										rm = stocks_daily_return['sp500'].mean() * 252
										rm

										# Assume risk free rate is zero (Used the yield of a 10-years U.S. Government bond as a risk free rate)
										rf = 0

										# Calculate return for any security (APPL) using CAPM
										Exp_return_AAPL = rf + (beta * (rm - rf))
										Exp_return_AAPL

										# Create a placeholder for all betas and alphas
										beta = {}
										alpha = {}

										for i in stocks_daily_return.columns[1:]:
										if i != 'sp500'and i != 'Date':

											stocks_daily_return.plot(kind = 'scatter', x = 'sp500', y = i, title = i)
											plt.scatter(stocks_daily_return['sp500'], stocks_daily_return[i])
											plt.xlabel('sp500')
											plt.ylabel(i)
											beta[i], alpha[i] = np.polyfit(stocks_daily_return['sp500'], stocks_daily_return[i], 1)
											plt.plot(stocks_daily_return['sp500'], beta[i] * stocks_daily_return['sp500'] + alpha[i], '-', color = 'r')

											plt.grid()
											plt.show()

											
										print('Beta for {} stock is {} & alpha is = {}'.format('AAPL', beta, alpha))


										# Apply CAPM formula to calculate the return for the Portfolio
										# Obtain a list of all stock names
										stock_names = list(beta.keys())
										stock_names

										# Define the expected return dictionary
										ER = {}

										rf = 0
										rm = stocks_daily_return['sp500'].mean() * 252
										for i in stock_names:
										ER[i] = rf + (beta[i] * (rm - rf))
										ER

										for i in stock_names:
										print('Expected return for {} is {}%'.format(i, ER[i]))

										Portfolio_weights = 1/8 * np.ones(8)

										# Assume equal weights in the portfolio, calculate returns
										ER_portfolio = sum(list(ER.values()) * Portfolio_weights)
										ER_portfolio
										</code></pre>

										
										<!-- Visualisations -->

										<div class="image-gallery">
										
										<figure>
											<a href="images/CAPM.png" data-lightbox="CAPM" data-title="Figure 1: Stock Daily Returns">
											<img src="images/CAPM.png" alt="Figure 1: Stock Daily Returns">
											</a>
											<figcaption><strong>Figure 1</strong></figcaption>
										</figure>
										
										</div>


										<h4>üåü <strong>Key Insights:</strong></h4>
										<ul>
											<li>Beta values revealed which stocks were more volatile than the market (beta > 1) versus defensive stocks (beta < 1)</li>
											<li>Alpha coefficients identified stocks generating excess returns above what CAPM predicted</li>
											<li>The regression analysis showed how closely each stock's movements correlated with overall market trends</li>
											<li>Visual scatter plots revealed the strength of linear relationships between individual stocks and market performance</li>
										</ul>

										<h4>üßóüèæ <strong>Challenge Faced:</strong></h4>
										<p>
											I initially struggled with the loop logic for batch processing all stocks because I was accidentally including the S&P 500 index in the analysis against itself, which created perfect correlation (beta = 1, alpha = 0) and distorted my results. After debugging, I realized I needed to exclude both 'sp500' and 'Date' columns using compound conditional statements <code>if i != 'sp500'</code> and <code>i != 'Date'</code>. This solution ensured I only analyzed actual stocks against the market benchmark, providing meaningful beta and alpha calculations for investment decision-making.
										</p>
										<p><a href="https://github.com/tendai-codes/your-project" class="github-button" target="_blank">View on GitHub</a></p>
									</section>

									<section class="project-item" data-tags="Python,Machine Learning">
										<h3>üìΩÔ∏è <strong>Startup Revenue Prediction Model  </strong></h3>
										<p>
											This project built a multiple linear regression model to predict startup profitability based on their R&D spending, administration costs, marketing expenditure, and location. I implemented a complete machine learning pipeline using scikit-learn to analyze which factors most strongly influence startup success and revenue generation.

										</p>

										<h4>üíª <strong>Tech Stack:</strong></h4>
										<ul>
											<li><strong>Python</strong> for machine learning model development</li>
											<li><strong>Scikit-learn</strong> for preprocessing, model training, and evaluation</li>
											<li><strong>Pandas</strong> for dataset loading and initial data exploration</li>
											<li><strong>Matplotlib</strong> for data visualisations</li>
											<li><strong>NumPy</strong> for numerical array operations and precision control</li>
											
										</ul>

										<h4>üß™ <strong>Data Pipeline:</strong></h4>
										<ul>
											<li><strong>Data import & separation:</strong> Loaded the 50 Startups dataset using <code>pd.read_csv()</code> and strategically separated features (X) from the target variable (y) using <code>.iloc[:, -1]</code> for all columns except the last, and <code>.iloc[:, -1]</code> for the dependent variable (profit). </li>
											<li><strong>Categorical Encoding:</strong> Applied One-Hot Encoding using <code>ColumnTransformer</code> and <code>OneHotEncoder()</code> to convert the categorical 'State' variable (column index [3]) into numerical dummy variables, while keeping other numerical features intact using <code>remainder='passthrough'</code>. </li>
											<li><strong>Data transformation, model training & prediction:</strong> Used <code>np.array(ct.fit_transform(X))</code> to convert the transformed data back into a NumPy array format suitable for machine learning algorithms. Implemented <code>train_test_split()</code> with an 80-20 split <code>(test_size=0.2)</code> and fixed random state <code>(random_state=0)</code> to ensure reproducible results and proper model validation. Instantiated and trained a <code>LinearRegression()</code> model using <code>.fit(X_train, y_train)</code> to learn the relationships between startup characteristics and profitability. Generated predictions on the test set using <code>regressor.predict(X_test)</code> to evaluate model performance on unseen data.</li>
											<li><strong>Results Visualisation:</strong> Used <code>np.set_printoptions(precision=2)</code> for clean output formatting and <code>np.concatenate()</code> with <code>reshape()</code> to create side-by-side comparison of predicted vs. actual values for easy performance assessment.</li>
											
										</ul>

										<h4>üìä <strong>Code Snippets & Visualisations:</strong></h4>

										<!-- Code Snippet -->

										<pre><code class="language-python"># Encoding categorical data 
										from sklearn.compose import ColumnTransformer
										from sklearn.preprocessing import OneHotEncoder

										# OneHotEncoder(), [3] - the 3 is the column you want to encode
										ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')
										X = np.array(ct.fit_transform(X))

										# Splitting Train ad Test set
										from sklearn.model_selection import train_test_split
										X_train, X_test, y_train, y_test = train_test_split(X, y, test_sise = 0.2, random_state = 0)

										from sklearn.linear_model import LinearRegression
										regressor = LinearRegression()
										regressor.fit(X_train, y_train)

										# Predicting Results
										y_pred = regressor.predict(X_test)
										np.set_printoptions(precision=2)
										print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

										</code></pre>



										<h4>üåü <strong>Key Insights:</strong></h4>
										<ul>
											<li>The model successfully learned to predict startup profitability based on spending patterns across R&D, administration, and marketing</li>
											<li>One-hot encoding effectively handled the categorical state variable, allowing the model to capture location-based effects on startup success</li>
											<li>The side-by-side prediction comparison revealed the model's accuracy in forecasting startup revenue</li>
											<li>Multiple linear regression proved effective for understanding the linear relationships between various business expenditures and profitability</li>
										</ul>

										<h4>üßóüèæ <strong>Challenge Faced:</strong></h4>
										<p>
											The array reshaping and concatenation for results display presented a significant hurdle because the predicted and actual values were 1D arrays that couldn't be directly concatenated horizontally. The error occurred when trying to use <code>np.concatenate()</code> without proper dimensionality. This was solved by using <code>reshape(len(y_pred),1)</code> to convert both arrays into column vectors (2D arrays with one column), then applying horizontal concatenation with the parameter 1 to stack them side-by-side. This approach created a clean comparison matrix showing predicted values next to actual values, making model performance evaluation much more intuitive.
										</p>
										<p><a href="https://github.com/tendai-codes/your-project" class="github-button" target="_blank">View on GitHub</a></p>
									</section>

									<section class="project-item" data-tags="Python,Machine Learning">
										<h3>üìΩÔ∏è <strong>HR Salary Prediction: Model Comparison </strong></h3>
										<p>
											This project implemented and compared five different regression algorithms to predict employee salaries based on position levels within an organization. I built a comprehensive machine learning pipeline comparing linear regression, polynomial regression, support vector regression (SVR), decision tree regression, and random forest regression to identify the optimal model for HR compensation analysis.
										</p>

										<h4>üíª <strong>Tech Stack:</strong></h4>
										<ul>
											<li><strong>Python</strong> for machine learning model development and comparison</li>
											<li><strong>Scikit-learn</strong> for multiple regression algorithms, feature scaling, and model training</li>
											<li><strong>Pandas</strong> for dataset loading and initial data exploration</li>
											<li><strong>Matplotlib</strong> for data visualisations</li>
											<li><strong>NumPy</strong> for numerical operations and grid generation</li>
										</ul>

										<h4>üß™ <strong>Data Pipeline:</strong></h4>
										<ul>
											<li><strong>Data Preparation:</strong> Loaded position-salary dataset using <code>pd.read_csv()</code> and extracted features using <code>iloc[:, 1:-1]</code> (position levels) and target variable using <code>iloc[:, -1]</code>  (salaries), strategically excluding the first column containing position titles. </li>
											<li><strong>Linear regression baseline:</strong> Implemented a standard<code>LinearRegression()</code>  model using <code>.fit(X, y)</code> to establish a baseline for salary prediction based on position level with a straight-line relationship. </li>
											<li><strong>Polynomial feature engineering: </strong> Applied <code>PolynomialFeatures(degree=4)</code>  to transform the single position level feature into polynomial terms (x, x¬≤, x¬≥, x‚Å¥), creating a richer feature space to capture non-linear salary progression patterns.</li>
											<li><strong>Support Vector Regression: </strong> Implemented feature scaling using <code>StandardScaler()</code>for both X and y variables, then trained an SVR model with <code>RBF kernel</code> to handle non-linear relationships while managing the different scales between position levels and salary amounts.</li>
											<li><strong>Decision Tree : </strong> Built a <code>DecisionTreeRegressor()</code> model that creates hierarchical decision rules to predict salaries, capturing complex non-linear patterns without requiring feature scaling.</li>
											<li><strong>Random Forest Regression: </strong> Implemented <code>RandomForestRegressor()</code> with multiple decision trees to reduce overfitting and improve prediction stability through ensemble learning.</li>
											<li><strong>Polynomial feature engineering: </strong> Applied <code>PolynomialFeatures(degree=4)</code>  to transform the single position level feature into polynomial terms (x, x¬≤, x¬≥, x‚Å¥), creating a richer feature space to capture non-linear salary progression patterns.</li>
											<li><strong>Random Forest Regression: </strong> Implemented <code>RandomForestRegressor()</code> with multiple decision trees to reduce overfitting and improve prediction stability through ensemble learning.</li>
											<li><strong>SVR Inverse scaling: </strong> Applied <code>sc_X.inverse_transform()</code> and <code>sc_y.inverse_transform()</code> to convert scaled predictions back to original salary units, with proper reshaping using <code>.reshape(-1, 1)</code> for visualization.</li>
										
											<li><strong>Model Visualisations:</strong> Created scatter plots with <code>plt.scatter()</code> for actual data points and <code>plt.plot()</code> for the linear regression line, showing the limitation of straight-line salary prediction. Generated similar visualizations for the polynomial model using <code>lin_reg_2.predict(poly_reg.fit_transform(X))</code> to display the curved relationship between position levels and salaries.</li>
											<li><strong>Model comparison:</strong> Made direct salary predictions for position level 6.5 using both <code>lin_reg.predict([[6.5]])</code> and <code>lin_reg_2.predict(poly_reg.fit_transform([[6.5]]))</code> to compare model outputs for intermediate position levels.</li>
										</ul>

										<h4>üìä <strong>Code Snippets & Visualisations:</strong></h4>

										<!-- Code Snippet -->

										<pre><code class="language-python"># Importing Libraries	
										import numpy as np
										import matplotlib.pyplot as plt
										import pandas as pd

										# Importing dataset	
										dataset = pd.read_csv('Position_Salaries.csv')
										X = dataset.iloc[:, 1:-1].values
										y = dataset.iloc[:, -1].values

										# Training the Decision Tree Regression
										from sklearn.tree import DecisionTreeRegressor
										regressor = DecisionTreeRegressor(random_state = 0)
										regressor.fit(X, y)

										# Predicting New Result
										regressor.predict([[6.5]])

										# Visualising Results (Figure 1)

										X_grid = np.arange(min(X), max(X), 0.01)
										# 0.01 was adjusted from 0.1 to increase the resolution
										X_grid = X_grid.reshape((len(X_grid), 1))
										plt.scatter(X, y, color = 'red')
										plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')
										plt.title('HR Salary Predictions (Decision Tree Regression)')
										plt.xlabel('Position level')
										plt.ylabel('Salary')
										plt.show()

										# Evaluating the Model Performance
										from sklearn.metrics import r2_score
										r2_score(y_test, y_pred)


										</code></pre>

										
										<!-- Visualisations -->

										<div class="image-gallery">
										<figure>
											<a href="images/HR_linReg.png" data-lightbox="salary-prediction" data-title="Figure 1: HR_linReg">
											<img src="images/HR_linReg.png" alt="Figure 1: HR_linReg">
											</a>
											<figcaption><strong>Figure 1</strong></figcaption>
										</figure>

										<figure>
											<a href="images/HR_polyReg.png" data-lightbox="salary-prediction" data-title="Figure 2: HR_polyReg">
											<img src="images/HR_polyReg.png" alt="Figure 2: HR_polyReg">
											</a>
											<figcaption><strong>Figure 2</strong></figcaption>
										</figure>

										<figure>
											<a href="images/HR_svr.png" data-lightbox="salary-prediction" data-title="Figure 3: HR_svr">
											<img src="images/HR_svr.png" alt="Figure 3: HR_svr">
											</a>
											<figcaption><strong>Figure 3</strong></figcaption>
										</figure>

										<figure>
											<a href="images/HR_Randfor.png" data-lightbox="salary-prediction" data-title="Figure 4: HR_Randfor">
											<img src="images/HR_Randfor.png" alt="Figure 4: HR_Randfor">
											</a>
											<figcaption><strong>Figure 4</strong></figcaption>
										</figure>
										
										<figure>
											<a href="images/HR_DecT.png" data-lightbox="salary-prediction" data-title="Figure 5: HR_DecT">
											<img src="images/HR_DecT.png" alt="Figure 5: HR_DecT">
											</a>
											<figcaption><strong>Figure 5</strong></figcaption>
										</figure>
										
										</div>


										<h4>üåü <strong>Key Insights:</strong></h4>
										<ul>
											<li>Linear regression showed limitations in capturing the exponential nature of executive compensation at higher position levels</li>
											<li>Polynomial regression successfully modeled smooth non-linear salary curves typical in corporate hierarchies</li>
											<li>SVR with proper scaling handled the high salary variance effectively while maintaining smooth predictions</li>
											<li>Decision tree regression captured salary jumps at specific position levels but risked overfitting</li>
											<li>Random forest regression provided stable predictions by averaging multiple decision trees, reducing variance</li>
										</ul>

										<h4>üßóüèæ <strong>Challenge Faced:</strong></h4>
										<p>
											
											The SVR model visualization presented scaling complications because support vector regression requires feature scaling for optimal performance, but the visualization needed to display results in original salary units. The challenge was handling the forward and inverse transformations correctly. This was resolved by implementing a multi-step process: using  <code>sc_X.transform(X_grid)</code> to scale the grid for SVR prediction, then applying <code>sc_y.inverse_transform()</code>  to convert predictions back to actual salary values, with careful attention to array reshaping using <code>.reshape(-1, 1)</code> to maintain proper dimensionality throughout the scaling pipeline. This approach ensured accurate model performance while maintaining interpretable visualizations in original salary units.
										</p>
										<p><a href="https://github.com/tendai-codes/your-project" class="github-button" target="_blank">View on GitHub</a></p>
									</section>

									<section class="project-item" data-tags="Python,Machine Learning">
										<h3>üìΩÔ∏è <strong>Breast Cancer Classification: Multi-Algorithm Comparison </strong></h3>
										<p>
											This project implemented and compared six different machine learning classification algorithms to predict breast cancer diagnosis (malignant vs benign) based on cellular characteristics. I built a comprehensive medical classification pipeline using multiple algorithms to identify the most effective approach for cancer detection and diagnosis support.
										</p>

										<h4>üíª <strong>Tech Stack:</strong></h4>
										<ul>
											<li><strong>Python</strong> for machine learning model development and comparison</li>
											<li><strong>Scikit-learn</strong> for multiple classification algorithms, preprocessing, and evaluation metrics</li>
											<li><strong>Pandas</strong> for dataset loading and initial data exploration</li>
											<li><strong>Matplotlib</strong> for data visualisationsand model performance analysis</li>
											<li><strong>NumPy</strong> for numerical operations and grid generation</li>
										</ul>

										<h4>üß™ <strong>Data Pipeline:</strong></h4>
										<ul>
											<li><strong>Load & inspect data:</strong> Loaded breast cancer dataset using <code>pd.read_csv()</code> and separated cellular features (X) from diagnosis labels (y) using <code>iloc[:, :-1]</code> and <code>iloc[:, -1]</code> respectively, ensuring proper handling of medical diagnostic data. </li>
											<li><strong>Train-test stratification:</strong> Applied <code>train_test_split()</code> with 75-25 split (<code>test_size=0.25</code>) and fixed random state for reproducible medical model evaluation, crucial for healthcare applications. </li>
											<li><strong>Feature standardization:</strong> Implemented <code>StandardScaler()</code> using <code>fit_transform()</code> on training data and <code>transform()</code> on test data to normalize cellular measurements across different scales without data leakage.</li>
											<li><strong>Logistic Regression: </strong> Built a <code>LogisticRegression(random_state=0)</code> model as the statistical baseline for binary medical classification, providing interpretable probability outputs for clinical decision-making.</li>
											<li><strong>Support Vector Machine (Linear):</strong> Implemented <code>SVC(kernel='linear')</code> to find optimal linear decision boundaries for separating malignant from benign cases using maximum margin principles.</li>
											<li><strong>Decision Tree Classification: </strong> Applied <code>DecisionTreeClassifier(criterion='entropy')</code> to create interpretable rule-based diagnostic pathways that clinicians can follow and understand.</li>
											<li><strong>K-Nearest Neighbors:</strong> Used <code>KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)</code> to classify cases based on similarity to neighboring data points, leveraging local patterns in cellular characteristics.</li>
											<li><strong>Support Vector Machine (RBF):</strong> Implemented <code>SVC(kernel='rbf')</code> with radial basis function kernel to capture complex non-linear relationships in cellular feature space.</li>
											<li><strong>Naive Bayes: </strong> Applied <code>GaussianNB()</code> assuming feature independence to provide probabilistic classification based on Bayesian statistics, suitable for medical diagnostic scenarios.</li>
											<li><strong>Performance evaluation: </strong> Generated predictions using <code>classifier.predict(X_test)</code> and evaluated each model using <code>confusion_matrix()</code> and <code>accuracy_score()</code> to assess diagnostic accuracy and error patterns.</li>
											<li><strong>Medical model validation: </strong> Created confusion matrices to analyze true positives, false positives, true negatives, and false negatives - critical metrics for medical diagnostic applications where false negatives (missed cancers) are particularly concerning..</li>

										</ul>

										<h4>üìä <strong>Code Snippets & Visualisations:</strong></h4>

										<!-- Code Snippet -->

										<pre><code class="language-python"># Importing the libraries
										import numpy as np
										import matplotlib.pyplot as plt
										import pandas as pd

										# Importing the dataset
										dataset = pd.read_csv('Breast cancer data.csv')
										X = dataset.iloc[:, :-1].values
										y = dataset.iloc[:, -1].values

										#Splitting the dataset into the Training set and Test set
										from sklearn.model_selection import train_test_split
										X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

										# Feature Scaling
										from sklearn.preprocessing import StandardScaler
										sc = StandardScaler()
										X_train = sc.fit_transform(X_train)
										X_test = sc.transform(X_test)

										# Training Model on the Training set
										from sklearn.tree import DecisionTreeClassifier
										classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
										classifier.fit(X_train, y_train)

										Evaluating using confusion matrix
										from sklearn.metrics import confusion_matrix, accuracy_score
										y_pred = classifier.predict(X_test)
										cm = confusion_matrix(y_test, y_pred)
										print(cm)
										accuracy_score(y_test, y_pred)

										</code></pre>

										
										<!-- Visualisations -->

										<div class="image-gallery">
										<figure>
											<a href="images/Breast_logR.png" data-lightbox="breast-cancer" data-title="Figure 1: Breast_logR">
											<img src="images/Breast_logR.png" alt="Figure 1: Breast_logR">
											</a>
											<figcaption><strong>Figure 1</strong> Logistic Regression</figcaption>
										</figure>

										<figure>
											<a href="images/Breast_svm.png" data-lightbox="breast-cancer" data-title="Figure 2: Breast_svm">
											<img src="images/Breast_svm.png" alt="Figure 2: Breast_svm">
											</a>
											<figcaption><strong>Figure 2</strong> SVM</figcaption>
										</figure>

										<figure>
											<a href="images/Breast_ksvm.png" data-lightbox="breast-cancer" data-title="Figure 3: Breast_ksvm">
											<img src="images/Breast_ksvm.png" alt="Figure 3: Breast_ksvm">
											</a>
											<figcaption><strong>Figure 3</strong> Kernel SVM</figcaption>
										</figure>

										<figure>
											<a href="images/Breast_knn.png" data-lightbox="breast-cancer" data-title="Figure 4: Breast_knn">
											<img src="images/Breast_knn.png" alt="Figure 4: Breast_knn">
											</a>
											<figcaption><strong>Figure 4</strong> K-Nearest Neighbor</figcaption>
										</figure>
										
										<figure>
											<a href="images/Breast_naive.png" data-lightbox="breast-cancer" data-title="Figure 5: Breast_naive">
											<img src="images/Breast_naive.png" alt="Figure 5: Breast_naive">
											</a>
											<figcaption><strong>Figure 5</strong> Na√Øve Bayes</figcaption>
										</figure>

										<figure>
											<a href="images/Breast_randfor.png" data-lightbox="breast-cancer" data-title="Figure 5: Breast_randfor">
											<img src="images/Breast_randfor.png" alt="Figure 2: Breast_randfor">
											</a>
											<figcaption><strong>Figure 6</strong> Random Forest</figcaption>
										</figure>
										
										<figure>
											<a href="images/Breast_decision.png" data-lightbox="breast-cancer" data-title="Figure 5: Breast_decision">
											<img src="images/Breast_decision.png" alt="Figure 2: Breast_decision">
											</a>
											<figcaption><strong>Figure 7</strong> Decision Tree</figcaption>
										</figure>
										</div>


										<h4>üåü <strong>Key Insights:</strong></h4>
										<ul>
											<li>Multiple algorithms provided different approaches to cancer classification, each with unique strengths for medical diagnosis</li>
											<li>Feature standardization proved crucial for distance-based algorithms (SVM, KNN) due to varying scales of cellular measurements</li>
											<li>Confusion matrix analysis revealed the trade-offs between sensitivity (detecting cancer) and specificity (avoiding false alarms)</li>
											<li>Model comparison enabled selection of the most reliable algorithm for medical diagnostic support</li>
										</ul>

										<h4>üßóüèæ <strong>Challenge Faced:</strong></h4>
										<p>
											Working with medical diagnostic data presented a critical class imbalance consideration that required careful attention to evaluation metrics beyond simple accuracy. While accuracy score provides an overall performance measure, it can be misleading in medical contexts where false negatives (missing actual cancer cases) have far more severe consequences than false positives (flagging benign cases as suspicious). The challenge was ensuring that model evaluation properly weighted the clinical importance of sensitivity (recall) versus specificity, as a model with 95% accuracy might still miss 20% of actual cancer cases if the dataset is imbalanced. This was addressed by implementing confusion matrix analysis to examine true positives, false positives, true negatives, and false negatives separately, enabling assessment of each model's ability to minimize the most clinically dangerous errors while maintaining overall diagnostic reliability.
										</p>
										<p><a href="https://github.com/tendai-codes/your-project" class="github-button" target="_blank">View on GitHub</a></p>
									</section>

									<section class="project-item" data-tags="Python,Machine Learning">
										<h3>üìΩÔ∏è <strong> Bank Customer Segmentation </strong></h3>
										<p>
											Analyzed bank customer data to segment customers using K-Means Clustering, with dimensionality reduction achieved through PCA. This approach resulted in 7 well-defined customer segments based on key financial behaviors, optimizing the bank's ability to tailor products and services.
										</p>

										<h4>üíª <strong>Tech Stack:</strong></h4>
										<ul>
											<li><strong>Python</strong> for machine learning model development and comparison</li>
											<li><strong>Scikit-learn</strong> for multiple classification algorithms, preprocessing, and evaluation metrics</li>
											<li><strong>Pandas</strong> for dataset loading and initial data exploration</li>
											<li><strong>Seaborn</strong> for enhanced statistical plotting capabilities</li>
											<li><strong>Matplotlib</strong> for data visualisationsand model performance analysis</li>
											<li><strong>NumPy</strong> for numerical operations and grid generation</li>
										</ul>

										<h4>üß™ <strong>Data Pipeline:</strong></h4>
										<ul>
											<li><strong>Load & inspect data:</strong> Imported multi-statement CSV into pandas. Extracted relevant sections for Balance Sheet, Income Statement, and Cash Flow Statement using <code>loc[]</code>. </li>
											<li><strong>Trend Analysis:</strong> Transposed time-series data <code>(.T)</code> and plotted major components (e.g. assets, equity) using <code>.plot()</code> to visualise financial stability over time. </li>
											<li><strong>Ratio Calculations:</strong> Computed solvency and profitability ratios: Return on Equity (ROE), Return on Assets (ROA), Debt-to-Equity. Built <code>DataFrame</code> to summarise and visualise using grouped bar plots.</li>
											<li><strong>Custom Metrics:</strong> Created Operating Cash Flow to Total Debt ratio from cash flow and balance sheet sections to assess short-term liquidity strength.</li>
											
										</ul>

										<h4>üìä <strong>Code Snippets & Visualisations:</strong></h4>

										<!-- Code Snippet -->

										<pre><code class="language-python"># Importing the libraries
										credit_card_df = pd.read_csv('/content/4.+Marketing_data.csv')
										credit_card_df.info()

										credit_card_df.describe() (Table 1)

										- Mean balance is $1564
										- Balance frequency is frequently updated on average ~0.9, Purchases average is $1000, one off purchase average is ~$600
										- Average purchases frequency is around 0.5, Average ONEOFF_PURCHASES_FREQUENCY, PURCHASES_INSTALLMENTS_FREQUENCY, and CASH_ADVANCE_FREQUENCY are generally low
										- Average credit limit ~ 4500, Percent of full payment is 15%, Average tenure is 11 years
										
										# Check for missing data (Table 2)
										credit_card_df.isnull().sum()

										# Replace the missing elements with mean of the 'MINIMUM_PAYMENT'
										credit_card_df.MINIMUM_PAYMENTS.fillna(credit_card_df.MINIMUM_PAYMENTS.mean(), inplace=True)

										# Replace the missing elements with mean of the 'CREDIT_LIMIT'
										credit_card_df.CREDIT_LIMIT.fillna(credit_card_df.CREDIT_LIMIT.mean(), inplace=True)

										# Plot to check for missing data (Figure 1)
										sns.heatmap(credit_card_df.isnull(), yticklabels = False, cbar = False, cmap='Reds')
										
										# Check for duplicate entries
										credit_card_df.duplicated().sum()

										# Remove Customer ID
										credit_card_df.drop('CUST_ID', axis=1, inplace=True)

										# Define function to create subplots of distplots with KDE for all columns.

										def dist_plots(dataframe):
										fig, ax = plt.subplots(nrows=7, ncols=2, figsize=(15,30))
										index = 0
										for row in range(7):
											for col in range(2):
											sns.distplot(dataframe.iloc[:, index], ax=ax[row][col], kde_kws={'color':'blue', 'lw':3, 'label':'KDE'}, hist_kws= {'histtype':'step', 'lw':3, 'color':'green'})
											index += 1
										plt.tight_layout()
										plt.show()

										# Visualise distplots (Figure 2)
										dist_plots(credit_card_df)
										
										- 'Balance_Frequency' for most customers is updated frequently ~1, For 'PURCHASES_FREQUENCY', there are two distinct group of customers
										- For 'ONEOFF_PURCHASES_FREQUENCY' and 'PURCHASES_INSTALLMENT_FREQUENCY' most users don't do one off puchases or installment purchases frequently, Very small number of customers pay their balance in full 'PRC_FULL_PAYMENT'~0
										- Mean of balance is $1500, Credit limit average is around $4500, Most customers are ~11 years tenure
										
										# Heatmap to visaulise correlations (Figure 3)
										correlations = credit_card_df.corr()
										plt.figure(figsize=(20,20))
										sns.heatmap(correlations, annot=True)

										- 'PURCHASES' have high correlation between one-off purchases, 'installment purchases, purchase transactions, credit limit and payments.
										- Strong Positive Correlation between 'PURCHASES_FREQUENCY' and 'PURCHASES_INSTALLMENT_FREQUENCY'

										# Training Model on the Training set
										from sklearn.tree import DecisionTreeClassifier
										classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
										classifier.fit(X_train, y_train)

										# Evaluating using confusion matrix
										from sklearn.metrics import confusion_matrix, accuracy_score
										y_pred = classifier.predict(X_test)
										cm = confusion_matrix(y_test, y_pred)
										print(cm)
										accuracy_score(y_test, y_pred)

										# Use Elbow Method to find optimal No of clusters (Figure 4)
										credit_card_df.head()

										# Apply Feature scaling
										scaler = StandardScaler()
										credit_card_df_scaled = scaler.fit_transform(credit_card_df)

										credit_card_df_scaled

										# Apply Elbow Method
										wcss = []
										for i in range(1, 20):
											kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
											kmeans.fit(credit_card_df_scaled)
											wcss.append(kmeans.inertia_)
										plt.plot(range(1, 20), wcss, 'bx-')
										plt.title('The Elbow Method')
										plt.xlabel('Number of clusters')
										plt.ylabel('Score (WCSS)')
										plt.show();

										# Train data using K-Means method 

										- We can observe that, 4th cluster seems to be forming the elbow of the curve. However, the values does not reduce linearly until 8th cluster. Let's choose the number of clusters to be 7.

										# Assuming 'kmeans' is your fitted KMeans model
										kmeans = KMeans(n_clusters=7, init= 'k-means++',random_state=42)
										kmeans.fit(credit_card_df_scaled)

										# Use Principal Compenent Analysis to reduce dimensionality
										pca = PCA(n_components=2)
										principalComp = pca.fit_transform(credit_card_df_scaled )
										principalComp

										# Create a dataframe with the two components
										pca_df = pd.DataFrame(data=principalComp, columns=['PCA1', 'PCA2'])
										pca_df

										# Concatenate the clusters labels to the dataframe (Table 3)
										pca_df = pd.concat([pca_df, pd.DataFrame({'cluster': labels})], axis=1)
										pca_df.head()

										# Visualise Clusters (Figure 5)
										plt.figure(figsize=(10, 10))
										ax = sns.scatterplot(x='PCA1', y='PCA2', hue='cluster', data=pca_df, palette='tab10')
										plt.title('Clusters identified by PCA')
										plt.show()
										</code></pre>

										
										<!-- Visualisations -->

										<div class="image-gallery">
										<figure>
											<a href="images/Bank Customer_df.png" data-lightbox="bank-customer" data-title="Table 1: DataFrame">
											<img src="images/Bank Customer_df.png" alt="Table 1: DataFrame">
											</a>
											<figcaption><strong>Table 1</strong> Data frame</figcaption>
										</figure>

										<figure>
											<a href="images/Bank Customer_miss.png" data-lightbox="bank-customer" data-title="Table 2: Missing data">
											<img src="images/Bank Customer_miss.png" alt="Table 2: Missing data">
											</a>
											<figcaption><strong>Table 2</strong> Missing data df</figcaption>
										</figure>

										<figure>
											<a href="images/Bank Customer_check missing.png" data-lightbox="bank-customer" data-title="Figure 1: check chart">
											<img src="images/Bank Customer_check missing.png" alt="Figure 1: check chart">
											</a>
											<figcaption><strong>Figure 1</strong> Check after removing missing data</figcaption>
										</figure>

										<figure>
											<a href="images/Bank Customer_distplots.png" data-lightbox="bank-customer" data-title="Figure 2: distplot">
											<img src="images/Bank Customer_distplots.png" alt="Figure 2: distplot">
											</a>
											<figcaption><strong>Figure 2</strong> Distplot</figcaption>
										</figure>
										
										<figure>
											<a href="images/Bank Customer_heat.png" data-lightbox="bank-customer" data-title="Figure 3: heatmap">
											<img src="images/Bank Customer_heat.png" alt="Figure 3: heatmap">
											</a>
											<figcaption><strong>Figure 3</strong> Heatmap</figcaption>
										</figure>

										<figure>
											<a href="images/Bank Customer_elbow.png" data-lightbox="bank-customer" data-title="Figure 4: Elbow Method">
											<img src="images/Bank Customer_elbow.png" alt="Figure 4: Elbow Method">
											</a>
											<figcaption><strong>Figure 4</strong> Elbow Method</figcaption>
										</figure>
										
										<figure>
											<a href="images/Bank Customer_table 3.png" data-lightbox="bank-customer" data-title="Table 3: PCA Table">
											<img src="images/Bank Customer_table 3.png" alt="Table 3: PCA Table">
											</a>
											<figcaption><strong>Table 3</strong> PCA Table</figcaption>
										</figure>
										
										<figure>
											<a href="images/bank customers clusters.png" data-lightbox="bank-customer" data-title="Figure 5: Bank Customer Clusters">
											<img src="images/bank customers clusters.png" alt="Figure 5: Bank Customer Clusters">
											</a>
											<figcaption><strong>Figure 5</strong> Bank Customer Clusters</figcaption>
										</figure>
										
										</div>

										<h4>üåü <strong>Key Insights:</strong></h4> #fill
										<ul>
											<li>Multiple algorithms provided different approaches to cancer classification, each with unique strengths for medical diagnosis</li>
											<li>Feature standardization proved crucial for distance-based algorithms (SVM, KNN) due to varying scales of cellular measurements</li>
											<li>Confusion matrix analysis revealed the trade-offs between sensitivity (detecting cancer) and specificity (avoiding false alarms)</li>
											<li>Model comparison enabled selection of the most reliable algorithm for medical diagnostic support</li>
										</ul>

										<h4>üßóüèæ <strong>Challenge Faced:</strong></h4> #fill
										<p>
											Initially, aligning the financial statements by year was inconsistent due to mixed string/index formats across categories. I overcame this by explicitly extracting year-based columns and standardising label references. This made ratio computations and cross-statement comparisons reliable and reproducible.
										</p>
										<p><a href="https://github.com/tendai-codes/your-project" class="github-button" target="_blank">View on GitHub</a></p>
									</section>

									<section class="project-item" data-tags="Python,Machine Learning,Tensorflow">
										<h3>üìΩÔ∏è <strong>Stock Price Analsysis using Ridge Regression </strong></h3>
										<p>
											Built a ridge regression model to predict stock prices with high accuracy. Leveraged Pandas for data preprocessing, and TensorFlow for model development. Achieved an R-squared score of 98%, with a k-fold cross-validation score of 86%.
										</p>

										<h4>üíª <strong>Tech Stack:</strong></h4>
										<ul>
											<li><strong>Python</strong> for machine learning model development and comparison</li>
											<li><strong>Scikit-learn</strong> for multiple classification algorithms, preprocessing, and evaluation metrics</li>
											<li><strong>Pandas</strong> for dataset loading and initial data exploration</li>
											<li><strong>Matplotlib</strong> for data visualisationsand model performance analysis</li>
											<li><strong>NumPy</strong> for numerical operations and grid generation</li>
											<li><strong>Matplotlib</strong> for static visualisations</li>
											<li><strong>Plotly</strong> for interactive charts and dashboards</li>
											<li><strong>NumPy & SciPy</strong> for statistical computations</li>
										</ul>

										<h4>üß™ <strong>Data Pipeline:</strong></h4>
										<ul>
											<li><strong>Load & inspect data:</strong> Imported multi-statement CSV into pandas. Extracted relevant sections for Balance Sheet, Income Statement, and Cash Flow Statement using <code>loc[]</code>. </li>
											<li><strong>Trend Analysis:</strong> Transposed time-series data <code>(.T)</code> and plotted major components (e.g. assets, equity) using <code>.plot()</code> to visualise financial stability over time. </li>
											<li><strong>Ratio Calculations:</strong> Computed solvency and profitability ratios: Return on Equity (ROE), Return on Assets (ROA), Debt-to-Equity. Built <code>DataFrame</code> to summarise and visualise using grouped bar plots.</li>
											<li><strong>Custom Metrics:</strong> Created Operating Cash Flow to Total Debt ratio from cash flow and balance sheet sections to assess short-term liquidity strength.</li>
											
										</ul>

										<h4>üìä <strong>Code Snippets & Visualisations:</strong></h4>

										<!-- Code Snippet -->

										<pre><code class="language-python"># Importing the libraries
										import pandas as pd
										import plotly.express as px
										from copy import copy
										from scipy import stats
										import matplotlib.pyplot as plt
										import numpy as np
										import plotly.figure_factory as ff
										from sklearn.linear_model import LinearRegression
										from sklearn.svm import SVR
										from sklearn.model_selection import train_test_split
										from sklearn.metrics import r2_score

										# Get stock prices dataframe info
										stock_price_df.info()

										# Get stock volume dataframe info
										stock_vol_df.info()

										stock_vol_df.describe()

										# Function to normalize stock prices based on their initial price (Figure 1)
										def normalize(df):
										x = df.copy()
										for i in x.columns[1:]:
											x[i] = x[i]/x[i][0]
										return x

										# Function to plot interactive plots using Plotly Express (Figure 2)
										def interactive_plot(df, title):
										fig = px.line(title = title)
										for i in df.columns[1:]:
											fig.add_scatter(x = df['Date'], y = df[i], name = i)
										fig.show()

										# Interactive chart for stocks Prices data (Figure 1)
										interactive_plot(stock_price_df, 'Stock Prices')

										# Normalised chart for stocks Prices data (Figure 2)
										interactive_plot(normalize(stock_price_df), 'Normalized Prices')

										# Interactive chart for stocks Volume data (Figure 3)
										interactive_plot(stock_vol_df, 'Stocks Volume')

										# Normalised chart for stocks volume data (Figure 4)
										interactive_plot(normalize(stock_vol_df), 'Normalized Volume')

										# Prepare Data before training Regression model

										# Function to concatenate the date, stock price, and volume in one dataframe
										def individual_stock(price_df, vol_df, name):
											return pd.DataFrame({'Date': price_df['Date'], 'Close': price_df[name], 'Volume': vol_df[name]})

										# Function to return the output (target) data ML Model [Target stock price today will be tomorrow's price]

										def trading_window(data):

										n = 1 #1 day window

										# Create a column containing the prices for the next 1 days
										data['Target'] = data[['Close']].shift(-n)

										return data

										# Test concatenation function using individual stock
										price_volume_df = individual_stock(stock_price_df, stock_vol_df, 'AAPL')
										price_volume_df

										# Function to return the output (target) data ML Model [Target stock price today will be tomorrow's price]
										
										def trading_window(data):

										n = 1 #1 day window

										# Create a column containing the prices for the next 1 days
										data['Target'] = data[['Close']].shift(-n)

										return data

										# Test concatenation function using individual stock
										price_volume_df = individual_stock(stock_price_df, stock_vol_df, 'AAPL')
										price_volume_df

										# Test trading window function using concatenated df (Table 1)
										price_volume_target_df = trading_window(price_volume_df)
										price_volume_target_df

										# Apply Feature Scaling to data
										from sklearn.preprocessing import MinMaxScaler
										sc = MinMaxScaler(feature_range = (0, 1))
										price_volume_target_scaled_df = sc.fit_transform(price_volume_target_df.drop(columns = ['Date']))

										# Creating Feature and Target
										X = price_volume_target_scaled_df[:,:2]
										y = price_volume_target_scaled_df[:,2:]

										# Converting dataframe to arrays
										X = np.asarray(X)
										y = np.asarray(y)
										X.shape, y.shape

										# Spliting the data this way, since order is important in time-series
										split = int(0.80 * len(X))
										X_train = X[:split]
										y_train = y[:split]
										X_test = X[split:]
										y_test = y[split:]

										# Define a data plotting function (Figue 5)
										def show_plot(data, title):
										plt.figure(figsize = (13, 5))
										plt.plot(data, linewidth = 3)
										plt.title(title)
										plt.grid()
										plt.legend(['X_train', 'X_test'])

										show_plot(X_train, 'Training Data for AAPL stock')
										show_plot(X_test, 'Testing Data for AAPL stock')

										# Build & Train Ridge Regression model
										- This model was chosen to get a generalised trend for data (avoids over fitting) - expected low testing accuracy expected. Note that Ridge regression performs linear least squares with L2 regularization.

										from sklearn.linear_model import Ridge
										regression_model = Ridge()
										regression_model.fit(X_train, y_train)

										# Test the model and calculate its accuracy
										ridge_accuracy = regression_model.score(X_test, y_test)
										print("Ridge Regression Score: ", ridge_accuracy)

										# K-Fold Cross validation Score
										from sklearn.model_selection import cross_val_score
										accuracies = cross_val_score(estimator = regression_model, X = X_train, y = y_train, cv = 10)
										print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
										print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

										Accuracy: 86.27%
										Standard Deviation: 11.01 %

										# Append the predicted values into a list
										predicted = []
										for i in predicted_prices:
										predicted.append(i[0])

										# Append the close values to the list
										close = []
										for i in price_volume_target_scaled_df:
											close.append(i[0])

										# Create a dataframe based on the dates in the individual stock data
										df_predicted = price_volume_target_df[['Date']]
										df_predicted

										# Function to add the close and predicted values to the dataframe
										def add_predicted_and_close(df, close, predicted):
										df_predicted['Close'] = close
										df_predicted['Prediction'] = predicted
										return df_predicted

										add_predicted_and_close(df_predicted, close, predicted) (Table 2)

										# Define interactive plot (Figure 6 & 7)

										def interactive_plot(df, title):
										fig = px.line(df, title = title, x='Date', y=['Close', 'Prediction'])

										fig.show()

										# Plot the results
										interactive_plot(df_predicted, "Original Vs. Prediction (alpha=1)")


										</code></pre>

										
										<!-- Visualisations -->

										<div class="image-gallery">
										<figure>
											<a href="images/Ridge_stock price.png" data-lightbox="ridge-regression" data-title="Figure 1: Stock Prices">
											<img src="images/Ridge_stock price.png" alt="Figure 1: Stock Prices ">
											</a>
											<figcaption><strong>Figure 1</strong> Stock Prices </figcaption>
										</figure>

										<figure>
											<a href="images/Ridge_stock price_norm.png" data-lightbox="ridge-regression" data-title="Figure 2: Stock Prices - Normalised">
											<img src="images/Ridge_stock price_norm.png" alt="Figure 2: Stock Prices - Normalised">
											</a>
											<figcaption><strong>Figure 2</strong> Stock Prices - Normalised</figcaption>
										</figure>
										<figure>
											<a href="images/Ridge_stock vol.png" data-lightbox="ridge-regression" data-title="Figure 3: Stock Volumes">
											<img src="images/Ridge_stock vol.png" alt="Figure 3: Stock Volumes">
											</a>
											<figcaption><strong>Figure 3</strong> Stock Volumes</figcaption>
										</figure>
										<figure>
											<a href="images/Ridge_stock vol_norm.png" data-lightbox="ridge-regression" data-title="Figure 3: Stock Volumes - Normalised">
											<img src="images/Ridge_stock vol_norm.png"  alt="Figure 3: Stock Volumes - Normalised">
											</a>
											<figcaption><strong>Figure 4</strong> Stock Volumes - Normalised</figcaption>
										</figure>

										<figure>
											<a href="images/Ridge_table 1.png" data-lightbox="ridge-regression" data-title="Figure 4: Price Volume Data">
											<img src="images/Ridge_table 1.png" alt="Figure 4: Price Volume Data">
											</a>
											<figcaption><strong>Table 1</strong> Price Volume Data</figcaption>
										</figure>

										<figure>
											<a href="images/Ridge_Figure 5.png" data-lightbox="ridge-regression" data-title="Figure 5: Training & Test Data">
											<img src="images/Ridge_Figure 5.png" alt="Figure 5: Closing & Predicted Values">
											</a>
											<figcaption><strong>Figure 5</strong> Training & Test Data</figcaption>
										</figure>
										
										<figure>
											<a href="images/Ridge_table 2.png" data-lightbox="ridge-regression" data-title="Figure 5: Closing & Predicted Values">
											<img src="images/Ridge_table 2.png" alt="Table 2: Closing & Predicted Values">
											</a>
											<figcaption><strong>Table 2</strong> Closing & Predicted Values</figcaption>
										</figure>

										<figure>
											<a href="images/Ridge_Figure 6.png" data-lightbox="ridge-regression" data-title="Figure 5: Breast_randfor">
											<img src="images/Ridge_Figure 6.png" alt="Figure 6: Breast_randfor">
											</a>
											<figcaption><strong>Figure 6</strong> Random Forest</figcaption>
										</figure>
										
										<figure>
											<a href="images/Ridge_Figure 7.png" data-lightbox="ridge-regression" data-title="Figure 5: Breast_decision">
											<img src="images/Ridge_Figure 7.png" alt="Figure 7: Breast_decision">
											</a>
											<figcaption><strong>Figure 7</strong> Decision Tree</figcaption>
										</figure>
										</div>

										<h4>üåü <strong>Key Insights:</strong></h4>
										<ul>
											<li>Multiple algorithms provided different approaches to cancer classification, each with unique strengths for medical diagnosis</li>
											<li>Feature standardization proved crucial for distance-based algorithms (SVM, KNN) due to varying scales of cellular measurements</li>
											<li>Confusion matrix analysis revealed the trade-offs between sensitivity (detecting cancer) and specificity (avoiding false alarms)</li>
											<li>Model comparison enabled selection of the most reliable algorithm for medical diagnostic support</li>
										</ul>

										<h4>üßóüèæ <strong>Challenge Faced:</strong></h4>
										<p>
											Initially, aligning the financial statements by year was inconsistent due to mixed string/index formats across categories. I overcame this by explicitly extracting year-based columns and standardising label references. This made ratio computations and cross-statement comparisons reliable and reproducible.
										</p>
										<p><a href="https://github.com/tendai-codes/your-project" class="github-button" target="_blank">View on GitHub</a></p>
									</section>

									<section class="project-item" data-tags="Python,TensorFlow,Deep Learning">
										<h3>üìΩÔ∏è <strong> Stock Price Analysis using Long Short Term Memory Neural Network </strong></h3>
										<p>
											Developed an LSTM neural network to predict stock price trends using a recurrent neural network structure. Preprocessed data using Pandas and constructed the model in TensorFlow. The model achieved an average validation loss of 0.000117 using k-fold cross-validation.
										</p>

										<h4>üíª <strong>Tech Stack:</strong></h4>
										<ul>
											<li><strong>Python</strong> for machine learning model development and comparison</li>
											<li><strong>Scikit-learn</strong> for multiple classification algorithms, preprocessing, and evaluation metrics</li>
											<li><strong>NumPy</strong> for numerical operations and grid generation</li>
											<li><strong>Pandas</strong> for dataset loading and initial data exploration</li>
											<li><strong>Matplotlib</strong> for data visualisationsand model performance analysis</li>
											<li><strong>TensorFlow</strong> for LSTM modelling</li>
										</ul>

										<h4>üß™ <strong>Data Pipeline:</strong></h4>
										<ul>
											<li><strong>Load & inspect data:</strong> Imported multi-statement CSV into pandas. Extracted relevant sections for Balance Sheet, Income Statement, and Cash Flow Statement using <code>loc[]</code>. </li>
											<li><strong>Trend Analysis:</strong> Transposed time-series data <code>(.T)</code> and plotted major components (e.g. assets, equity) using <code>.plot()</code> to visualise financial stability over time. </li>
											<li><strong>Ratio Calculations:</strong> Computed solvency and profitability ratios: Return on Equity (ROE), Return on Assets (ROA), Debt-to-Equity. Built <code>DataFrame</code> to summarise and visualise using grouped bar plots.</li>
											<li><strong>Custom Metrics:</strong> Created Operating Cash Flow to Total Debt ratio from cash flow and balance sheet sections to assess short-term liquidity strength.</li>
											
										</ul>

										<h4>üìä <strong>Code Snippets & Visualisations:</strong></h4>

										<!-- Code Snippet -->

										<pre><code class="language-python"># Importing the libraries
										import pandas as pd
										import plotly.express as px
										from copy import copy
										from scipy import stats
										import matplotlib.pyplot as plt
										import numpy as np
										import plotly.figure_factory as ff
										from sklearn.linear_model import LinearRegression
										from sklearn.svm import SVR
										from sklearn.model_selection import train_test_split
										from sklearn.metrics import r2_score
										from tensorflow import keras

										# Prepare Data for training

										# Function to concatenate the date, stock price, and volume in one dataframe
										def individual_stock(price_df, vol_df, name):
											return pd.DataFrame({'Date': price_df['Date'], 'Close': price_df[name], 'Volume': vol_df[name]})

										# Function to return the output (target) data ML Model [Target stock price today will be tomorrow's price]

										def trading_window(data):

										n = 1 #1 day window

										# Create a column containing the prices for the next 1 days
										data['Target'] = data[['Close']].shift(-n)

										return data

										# Test concatenation function using individual stock (Table 1)
										price_volume_df = individual_stock(stock_price_df, stock_vol_df, 'TSLA')
										price_volume_df

										# Train An LSTM Time Series Model

										# Use the close and volume data as training data (Input)
										training_data = price_volume_df.iloc[:, 1:3].values
										training_data

										# Apply feature scaling the data
										from sklearn.preprocessing import MinMaxScaler
										sc = MinMaxScaler(feature_range = (0, 1))
										training_set_scaled = sc.fit_transform(training_data)

										# Create the training and testing data, training data contains present day and previous day values
										X = []
										y = []
										for i in range(1, len(price_volume_df)):
											X.append(training_set_scaled [i-1:i, 0])
											y.append(training_set_scaled [i, 0])

										# Convert the data into array format
										X = np.asarray(X)
										y = np.asarray(y)

										# Split the data
										split = int(0.75 * len(X))
										X_train = X[:split]
										y_train = y[:split]
										X_test = X[split:]
										y_test = y[split:]

										# Reshape the 1D arrays to 3D arrays to feed in the model
										X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
										X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
										X_train.shape, X_test.shape

										# Create the LSTM model (Table 1)
										inputs = keras.layers.Input(shape=(X_train.shape[1], X_train.shape[2]))
										x = keras.layers.LSTM(150, return_sequences= True)(inputs)
										x = keras.layers.Dropout(0.3)(x)

										x = keras.layers.LSTM(150, return_sequences=True)(x)
										x = keras.layers.Dropout(0.3)(x)

										x = keras.layers.LSTM(150)(x)
										outputs = keras.layers.Dense(1, activation='linear')(x)

										model = keras.Model(inputs=inputs, outputs=outputs)
										model.compile(optimizer='adam', loss="mse")
										model.summary()

										# Train the model
										history = model.fit(
											X_train, y_train,
											epochs = 20,
											batch_size = 32,
											validation_split = 0.2)

										# K folds cross validation for model

										from sklearn.model_selection import KFold

										# Define the number of folds
										k = 5
										kf = KFold(n_splits=k, shuffle=True)

										# Initialize a list to store the validation loss for each fold
										fold_losses = []

										for train_index, val_index in kf.split(X):
										X_train_fold, X_val_fold = X[train_index], X[val_index]
										y_train_fold, y_val_fold = y[train_index], y[val_index]

										# Reshape data for LSTM
										X_train_fold = np.reshape(X_train_fold, (X_train_fold.shape[0], X_train_fold.shape[1], 1))
										X_val_fold = np.reshape(X_val_fold, (X_val_fold.shape[0], X_val_fold.shape[1], 1))

										# Train the model on the training fold
										history = model.fit(
											X_train_fold, y_train_fold,
											epochs=20,
											batch_size=32,
											validation_data=(X_val_fold, y_val_fold)	)

										# Evaluate the model on the validation fold and store the loss
										val_loss = model.evaluate(X_val_fold, y_val_fold)
										fold_losses.append(val_loss)

										# Calculate the average validation loss across all folds
										average_val_loss = np.mean(fold_losses)
										print("Average Validation Loss:", average_val_loss)

										# Get the Closing price data
										close = []
										for i in training_set_scaled:
										close.append(i[0])

										# Create dataframe for the dates, predicted prices and closing price
										df_predicted = price_volume_df[1:][['Date']]
										df_predicted['Predictions'] = test_predicted
										df_predicted['Close'] = close[1:]
										df_predicted

										# Define interactive plot

										def interactive_plot(df, title):
										fig = px.line(df, title = title, x='Date', y=['Close', 'Predictions'])

										fig.show()

										# Plot the results (Figure 1 & 2)
										interactive_plot(df_predicted, "Original Vs. Prediction (TSLA)")

										</code></pre>

										
										<!-- Visualisations -->

										<div class="image-gallery">
										<figure>
											<a href="images/LSTM_table 1.png" data-lightbox="LSTM" data-title="Table 1: closing vol Df">
											<img src="images/LSTM_table 1.png" alt="Table 1: closing vol Df">
											</a>
											<figcaption><strong>Table 1</strong></figcaption>
										</figure>

										<figure>
											<a href="images/LSTM_table 2.png" data-lightbox="LSTM" data-title="Table 2: LSTM Model Execution">
											<img src="images/LSTM_table 2.png" alt="Table 2: LSTM Model Execution">
											</a>
											<figcaption><strong>Table 2</strong> LSTM Model Execution</figcaption>
										</figure>

										<figure>
											<a href="images/LSTM_table 3.png" data-lightbox="LSTM" data-title="Table 3: Closing vs Prediction Price">
											<img src="images/LSTM_table 3.png" alt="Table 3: Closing vs Prediction Price">
											</a>
											<figcaption><strong>Table 3</strong> Closing vs Prediction Price</figcaption>
										</figure>

										<figure>
											<a href="images/LSTM_fig 1.png" data-lightbox="LSTM" data-title="Figure 1: Closing Price">
											<img src="images/LSTM_fig 1.png" alt="Figure 1: Closing Price">
											</a>
											<figcaption><strong>Figure 1</strong> Closing Price</figcaption>
										</figure>
										
										<figure>
											<a href="images/LSTM_fig 2.png" data-lightbox="LSTM" data-title="Figure 2: Closing vs Prediction Price">
											<img src="images/LSTM_fig 2.png" alt="Figure 2: Closing vs Prediction Price">
											</a>
											<figcaption><strong>Figure 2</strong> Closing vs Prediction Price</figcaption>
										</figure>

										</div>

										<h4>üåü <strong>Key Insights:</strong></h4>
										<ul>
											<li>Multiple algorithms provided different approaches to cancer classification, each with unique strengths for medical diagnosis</li>
											<li>Feature standardization proved crucial for distance-based algorithms (SVM, KNN) due to varying scales of cellular measurements</li>
											<li>Confusion matrix analysis revealed the trade-offs between sensitivity (detecting cancer) and specificity (avoiding false alarms)</li>
											<li>Model comparison enabled selection of the most reliable algorithm for medical diagnostic support</li>
										</ul>

										<h4>üßóüèæ <strong>Challenge Faced:</strong></h4>
										<p>
											Initially, aligning the financial statements by year was inconsistent due to mixed string/index formats across categories. I overcame this by explicitly extracting year-based columns and standardising label references. This made ratio computations and cross-statement comparisons reliable and reproducible.
										</p>
										<p><a href="https://github.com/tendai-codes/your-project" class="github-button" target="_blank">View on GitHub</a></p>
									</section>

									<section class="project-item" data-tags="Python,Deep Learning,TensorFlow">
										<h3>üìΩÔ∏è <strong>Building an ANN to predict Bank customer churn (Classification) </strong></h3>
										<p>
											This project explored publicly available data on top-grossing US films to identify patterns across genres, studios, and release schedules. I used <strong>R</strong> and <strong>ggplot2</strong> for data exploration and visualisation.
										</p>

										<h4>üíª <strong>Tech Stack:</strong></h4>
										<ul>
											<li><strong>Python</strong> for machine learning model development and comparison</li>
											<li><strong>Scikit-learn</strong> for multiple classification algorithms, preprocessing, and evaluation metrics</li>
											<li><strong>Pandas</strong> for dataset loading and initial data exploration</li>
											<li><strong>Matplotlib</strong> for data visualisationsand model performance analysis</li>
											<li><strong>NumPy</strong> for numerical operations and grid generation</li>
										</ul>

										<h4>üß™ <strong>Data Pipeline:</strong></h4>
										<ul>
											<li><strong>Load & inspect data:</strong> Imported multi-statement CSV into pandas. Extracted relevant sections for Balance Sheet, Income Statement, and Cash Flow Statement using <code>loc[]</code>. </li>
											<li><strong>Trend Analysis:</strong> Transposed time-series data <code>(.T)</code> and plotted major components (e.g. assets, equity) using <code>.plot()</code> to visualise financial stability over time. </li>
											<li><strong>Ratio Calculations:</strong> Computed solvency and profitability ratios: Return on Equity (ROE), Return on Assets (ROA), Debt-to-Equity. Built <code>DataFrame</code> to summarise and visualise using grouped bar plots.</li>
											<li><strong>Custom Metrics:</strong> Created Operating Cash Flow to Total Debt ratio from cash flow and balance sheet sections to assess short-term liquidity strength.</li>
											
										</ul>

										<h4>üìä <strong>Code Snippets & Visualisations:</strong></h4>

										<!-- Code Snippet -->

										<pre><code class="language-python"># Importing the libraries
										import numpy as np
										import pandas as pd
										import tensorflow as tf

										# Import dataset
										dataset = pd.read_csv('Churn_Modelling.csv')
										X = dataset.iloc[:, 3:-1].values
										y = dataset.iloc[:, -1].values

										print(X) (Table 1)
										print(y) (Table 2)

										# Encoding categorical data (encoding gender column) (Table 3)
										from sklearn.preprocessing import LabelEncoder
										le = LabelEncoder()
										X[:, 2] = le.fit_transform(X[:, 2])

										print(X)

										# Encoding categorical data (One Hot encoding geography column) (Table 4)
										from sklearn.compose import ColumnTransformer
										from sklearn.preprocessing import OneHotEncoder
										ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')
										X = np.array(ct.fit_transform(X))

										print(X)

										# Splitting the dataset into Training and test set
										from sklearn.model_selection import train_test_split
										X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

										# Feature Scaling
										from sklearn.preprocessing import StandardScaler
										sc = StandardScaler()
										X_train = sc.fit_transform(X_train)
										X_test = sc.transform(X_test)

										# Building ANN
										ann = tf.keras.models.Sequential()
										ann.add(tf.keras.layers.Dense(units=6, activation='relu'))
										ann.add(tf.keras.layers.Dense(units=6, activation='relu'))
										ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

										# Compiling the ANN
										ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

										# Training ANN (Table 5)
										ann.fit(X_train, y_train, batch_size = 32, epochs = 100)

										# Predicting results of a single observation
										print(ann.predict(sc.transform([[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])) > 0.5)

										# Predicting Test set results (Table 6)
										y_pred = ann.predict(X_test)
										y_pred = (y_pred > 0.5)
										print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

										# Making the Confusion Matrix (Table 7)

										from sklearn.metrics import confusion_matrix, accuracy_score
										cm = confusion_matrix(y_test, y_pred)
										print(cm)
										accuracy_score(y_test, y_pred)

										</code></pre>

										
										<!-- Visualisations -->

										<div class="image-gallery">
										<figure>
											<a href="images/ANNClass_table 1.png" data-lightbox="ANN-Class" data-title="Table 1: Breast_logR">
											<img src="images/ANNClass_table 1.png" alt="Table 1: Breast_logR">
											</a>
											<figcaption><strong>Table 1</strong> Variable X</figcaption>
										</figure>

										<figure>
											<a href="images/ANNClass_table 2.png" data-lightbox="ANN-Class" data-title="Table 2: Breast_svm">
											<img src="images/ANNClass_table 2.png" alt="Table 2: Breast_svm">
											</a>
											<figcaption><strong>Table 2</strong> Variable y</figcaption>
										</figure>

										<figure>
											<a href="images/ANNClass_table 3.png" data-lightbox="ANN-Class" data-title="Table 3: Breast_ksvm">
											<img src="images/ANNClass_table 3.png" alt="Table 3: Breast_ksvm">
											</a>
											<figcaption><strong>Table 3</strong> Encoding Gender</figcaption>
										</figure>

										<figure>
											<a href="images/ANNClass_table 4.png" data-lightbox="ANN-Class" data-title="Table 4: Breast_knn">
											<img src="images/ANNClass_table 4.png" alt="Table 4: Breast_knn">
											</a>
											<figcaption><strong>Table 4</strong> Encoding Country</figcaption>
										</figure>
										
										<figure>
											<a href="images/ANNClass_table 5.png" data-lightbox="ANN-Class" data-title="Table 5: Breast_naive">
											<img src="images/ANNClass_table 5.png" alt="Table 5: Breast_naive">
											</a>
											<figcaption><strong>Table 5</strong> Training the ANN</figcaption>
										</figure>

										<figure>
											<a href="images/ANNClass_table 6.png" data-lightbox="ANN-Class" data-title="Table 6: Breast_randfor">
											<img src="images/ANNClass_table 6.png" alt="Table 6: Breast_randfor">
											</a>
											<figcaption><strong>Table 6</strong> Predicting Test set Results</figcaption>
										</figure>
										
										<figure>
											<a href="images/ANNClass_table 7.png" data-lightbox="ANN-Class" data-title="Table 7: Breast_decision">
											<img src="images/ANNClass_table 7.png" alt="Table 7: Breast_decision">
											</a>
											<figcaption><strong>Table 7</strong> Confusion Matrix</figcaption>
										</figure>
										</div>

										<h4>üåü <strong>Key Insights:</strong></h4>
										<ul>
											<li>Multiple algorithms provided different approaches to cancer classification, each with unique strengths for medical diagnosis</li>
											<li>Feature standardization proved crucial for distance-based algorithms (SVM, KNN) due to varying scales of cellular measurements</li>
											<li>Confusion matrix analysis revealed the trade-offs between sensitivity (detecting cancer) and specificity (avoiding false alarms)</li>
											<li>Model comparison enabled selection of the most reliable algorithm for medical diagnostic support</li>
										</ul>

										<h4>üßóüèæ <strong>Challenge Faced:</strong></h4>
										<p>
											Initially, aligning the financial statements by year was inconsistent due to mixed string/index formats across categories. I overcame this by explicitly extracting year-based columns and standardising label references. This made ratio computations and cross-statement comparisons reliable and reproducible.
										</p>
										<p><a href="https://github.com/tendai-codes/your-project" class="github-button" target="_blank">View on GitHub</a></p>
									</section>

									<section class="project-item" data-tags="Python,Deep Learning,TensorFlow">
										<h3>üìΩÔ∏è <strong>Building an ANN to predict... (Regression) </strong></h3>
										<p>
											This project explored publicly available data on top-grossing US films to identify patterns across genres, studios, and release schedules. I used <strong>R</strong> and <strong>ggplot2</strong> for data exploration and visualisation.
										</p>

										<h4>üíª <strong>Tech Stack:</strong></h4>
										<ul>
											<li><strong>Python</strong> for machine learning model development and comparison</li>
											<li><strong>Scikit-learn</strong> for multiple classification algorithms, preprocessing, and evaluation metrics</li>
											<li><strong>Pandas</strong> for dataset loading and initial data exploration</li>
											<li><strong>Matplotlib</strong> for data visualisationsand model performance analysis</li>
											<li><strong>NumPy</strong> for numerical operations and grid generation</li>
										</ul>

										<h4>üß™ <strong>Data Pipeline:</strong></h4>
										<ul>
											<li><strong>Load & inspect data:</strong> Imported multi-statement CSV into pandas. Extracted relevant sections for Balance Sheet, Income Statement, and Cash Flow Statement using <code>loc[]</code>. </li>
											<li><strong>Trend Analysis:</strong> Transposed time-series data <code>(.T)</code> and plotted major components (e.g. assets, equity) using <code>.plot()</code> to visualise financial stability over time. </li>
											<li><strong>Ratio Calculations:</strong> Computed solvency and profitability ratios: Return on Equity (ROE), Return on Assets (ROA), Debt-to-Equity. Built <code>DataFrame</code> to summarise and visualise using grouped bar plots.</li>
											<li><strong>Custom Metrics:</strong> Created Operating Cash Flow to Total Debt ratio from cash flow and balance sheet sections to assess short-term liquidity strength.</li>
											
										</ul>

										<h4>üìä <strong>Code Snippets & Visualisations:</strong></h4>

										<!-- Code Snippet -->

										<pre><code class="language-python"># Importing the libraries
										import numpy as np
										import pandas as pd
										import tensorflow as tf

										# Importing dataset
										dataset = pd.read_excel('Folds5x2_pp.xlsx')
										X = dataset.iloc[:, :-1].values
										y = dataset.iloc[:, -1].values

										# Splitting the dataset into the Training set and Test set

										from sklearn.model_selection import train_test_split
										X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

										# Building the ANN

										ann = tf.keras.models.Sequential()
										ann.add(tf.keras.layers.Dense(units=6, activation='relu'))
										ann.add(tf.keras.layers.Dense(units=6, activation='relu'))
										ann.add(tf.keras.layers.Dense(units=6, activation='relu'))
										ann.add(tf.keras.layers.Dense(units=1))

										# Compiling the ANN
										ann.compile(optimizer = 'adam', loss = 'mean_squared_error')

										# Training the ANN (Table 1)

										ann.fit(X_train, y_train, batch_size = 32, epochs = 100)

										# Predicting the results of the test set (Table 2)
										y_pred = ann.predict(X_test)
										np.set_printoptions(precision=2)
										print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

										</code></pre>

										
										<!-- Visualisations -->

										<div class="image-gallery">
										<figure>
											<a href="images/ANNReg_Table 1.png" data-lightbox="ANN-Reg" data-title="Table 1: Training ANN">
											<img src="images/ANNReg_Table 1.png" alt="Table 1: Training ANN">
											</a>
											<figcaption><strong>Table 1</strong> Training ANN</figcaption>
										</figure>

										<figure>
											<a href="images/ANNReg_Table 2.png" data-lightbox="ANN-Reg" data-title="Table 2: Predicting Test Set Results">
											<img src="images/ANNReg_Table 2.png" alt="Table 2: Predicting Test Set Results">
											</a>
											<figcaption><strong>Table 2</strong> Predicting Test Set Results</figcaption>
										</figure>

										<figure>
											<a href="images/ANNClass_table 3.png" data-lightbox="ANN-Reg" data-title="Table 3: Breast_ksvm">
											<img src="images/ANNClass_table 3.png" alt="Table 3: Breast_ksvm">
											</a>
											<figcaption><strong>Table 3</strong> Encoding Gender</figcaption>
										</figure>

										<figure>
											<a href="images/ANNClass_table 4.png" data-lightbox="ANN-Reg" data-title="Table 4: Breast_knn">
											<img src="images/ANNClass_table 4.png" alt="Table 4: Breast_knn">
											</a>
											<figcaption><strong>Table 4</strong> Encoding Country</figcaption>
										</figure>
										
										</div>

										<h4>üåü <strong>Key Insights:</strong></h4>
										<ul>
											<li>Multiple algorithms provided different approaches to cancer classification, each with unique strengths for medical diagnosis</li>
											<li>Feature standardization proved crucial for distance-based algorithms (SVM, KNN) due to varying scales of cellular measurements</li>
											<li>Confusion matrix analysis revealed the trade-offs between sensitivity (detecting cancer) and specificity (avoiding false alarms)</li>
											<li>Model comparison enabled selection of the most reliable algorithm for medical diagnostic support</li>
										</ul>

										<h4>üßóüèæ <strong>Challenge Faced:</strong></h4>
										<p>
											Initially, aligning the financial statements by year was inconsistent due to mixed string/index formats across categories. I overcame this by explicitly extracting year-based columns and standardising label references. This made ratio computations and cross-statement comparisons reliable and reproducible.
										</p>
										<p><a href="https://github.com/tendai-codes/your-project" class="github-button" target="_blank">View on GitHub</a></p>
									</section>

									<section class="project-item" data-tags="Python,Deep Learning,TensorFlow">
										<h3>üìΩÔ∏è <strong>Natural Language Processing Model to understand resturant reviews </strong></h3>
										<p>
											This project explored publicly available data on top-grossing US films to identify patterns across genres, studios, and release schedules. I used <strong>R</strong> and <strong>ggplot2</strong> for data exploration and visualisation.
										</p>

										<h4>üíª <strong>Tech Stack:</strong></h4>
										<ul>
											<li><strong>Python</strong> for machine learning model development and comparison</li>
											<li><strong>Scikit-learn</strong> for multiple classification algorithms, preprocessing, and evaluation metrics</li>
											<li><strong>Pandas</strong> for dataset loading and initial data exploration</li>
											<li><strong>Matplotlib</strong> for data visualisationsand model performance analysis</li>
											<li><strong>NumPy</strong> for numerical operations and grid generation</li>
										</ul>

										<h4>üß™ <strong>Data Pipeline:</strong></h4>
										<ul>
											<li><strong>Load & inspect data:</strong> Imported multi-statement CSV into pandas. Extracted relevant sections for Balance Sheet, Income Statement, and Cash Flow Statement using <code>loc[]</code>. </li>
											<li><strong>Trend Analysis:</strong> Transposed time-series data <code>(.T)</code> and plotted major components (e.g. assets, equity) using <code>.plot()</code> to visualise financial stability over time. </li>
											<li><strong>Ratio Calculations:</strong> Computed solvency and profitability ratios: Return on Equity (ROE), Return on Assets (ROA), Debt-to-Equity. Built <code>DataFrame</code> to summarise and visualise using grouped bar plots.</li>
											<li><strong>Custom Metrics:</strong> Created Operating Cash Flow to Total Debt ratio from cash flow and balance sheet sections to assess short-term liquidity strength.</li>
											
										</ul>

										<h4>üìä <strong>Code Snippets & Visualisations:</strong></h4>

										<!-- Code Snippet -->

										<pre><code class="language-python"># Importing the libraries
										import re
										import nltk
										nltk.download('stopwords')
										from nltk.corpus import stopwords
										from nltk.stem.porter import PorterStemmer
										corpus = []
										for i in range(0, 1000):
										review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])
										review = review.lower()
										review = review.split()
										ps = PorterStemmer()
										all_stopwords = stopwords.words('english')
										all_stopwords.remove('not')
										review = [ps.stem(word) for word in review if not word in set(all_stopwords)]
										review = ' '.join(review)
										corpus.append(review)

										print(corpus) (Table 1)

										# Creating the bag of words model
										from sklearn.feature_extraction.text import CountVectorizer
										cv = CountVectorizer(max_features = 1500)
										X = cv.fit_transform(corpus).toarray()
										y = dataset.iloc[:, -1].values

										# Splitting the dataset into the Training set and Test set

										from sklearn.model_selection import train_test_split
										X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)

										# Training the Naive Bayes model on the Training set (Table 2)
										from sklearn.naive_bayes import GaussianNB
										classifier = GaussianNB()
										classifier.fit(X_train, y_train)

										# Predicting the Test set results (Table 3)
										y_pred = classifier.predict(X_test)
										print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

										# Making Confusion Matrix (Table 4)
										from sklearn.metrics import confusion_matrix, accuracy_score
										cm = confusion_matrix(y_test, y_pred)
										print(cm)
										accuracy_score(y_test, y_pred)
										</code></pre>

										
										<!-- Visualisations -->

										<div class="image-gallery">
										<figure>
											<a href="images/NLP_table 1.png" data-lightbox="NLP" data-title="Table 1: Removed words">
											<img src="images/NLP_table 1.png" alt="Table 1: Removed words">
											</a>
											<figcaption><strong>Table 1</strong> Removed words</figcaption>
										</figure>

										<figure>
											<a href="images/NLP_table 2.png" data-lightbox="NLP" data-title="Table 2: Naive Bayes Model">
											<img src="images/NLP_table 2.png" alt="Table 2: Naive Bayes Model">
											</a>
											<figcaption><strong>Table 2</strong> Naive Bayes Model y</figcaption>
										</figure>

										<figure>
											<a href="images/NLP_table 3.png" data-lightbox="NLP" data-title="Table 3: Predicting Test Set Results">
											<img src="images/NLP_table 3.png" alt="Table 3: Predicting Test Set Results">
											</a>
											<figcaption><strong>Table 3</strong> Predicting Test Set Results </figcaption>
										</figure>

										<figure>
											<a href="images/NLP_table 4.png" data-lightbox="NLP" data-title="Table 4: Confusion Matrix">
											<img src="images/NLP_table 4.png" alt="Table 4: Confusion Matrix">
											</a>
											<figcaption><strong>Table 4</strong> Confusion Matrix </figcaption>
										</figure>
										
										<figure>
											<a href="images/" data-lightbox="NLP" data-title="Figure 5: Breast_naive">
											<img src="images/" alt="Figure 5: Breast_naive">
											</a>
											<figcaption><strong>Figure 5</strong> Na√Øve Bayes</figcaption>
										</figure>

										<figure>
											<a href="images/" data-lightbox="NLP" data-title="Figure 5: Breast_randfor">
											<img src="images/" alt="Figure 2: Breast_randfor">
											</a>
											<figcaption><strong>Figure 6</strong> Random Forest</figcaption>
										</figure>
									
										</div>

										<h4>üåü <strong>Key Insights:</strong></h4>
										<p>
											The company showed stable asset growth and rising equity, but the Debt-to-Equity ratio increased post-2020, signalling higher leverage risk.
											Operating cash flow was consistently positive, suggesting sufficient liquidity to meet short-term obligations ‚Äî a green flag for operational health.
										</p>

										<h4>üßóüèæ <strong>Challenge Faced:</strong></h4>
										<p>
											Initially, aligning the financial statements by year was inconsistent due to mixed string/index formats across categories. I overcame this by explicitly extracting year-based columns and standardising label references. This made ratio computations and cross-statement comparisons reliable and reproducible.
										</p>
										<p><a href="https://github.com/tendai-codes/your-project" class="github-button" target="_blank">View on GitHub</a></p>
									</section>

									<section class="project-item" data-tags="Python,Deep Learning,TensorFlow">
										<h3>üìΩÔ∏è <strong>Building Convolutional Neural network to differentiate between dogs and cats (Classification) </strong></h3>
										<p>
											This project explored publicly available data on top-grossing US films to identify patterns across genres, studios, and release schedules. I used <strong>R</strong> and <strong>ggplot2</strong> for data exploration and visualisation.
										</p>

										<h4>üíª <strong>Tech Stack:</strong></h4>
										<ul>
											<li><strong>Python</strong> for machine learning model development and comparison</li>
											<li><strong>Scikit-learn</strong> for multiple classification algorithms, preprocessing, and evaluation metrics</li>
											<li><strong>Pandas</strong> for dataset loading and initial data exploration</li>
											<li><strong>Matplotlib</strong> for data visualisationsand model performance analysis</li>
											<li><strong>NumPy</strong> for numerical operations and grid generation</li>
										</ul>

										<h4>üß™ <strong>Data Pipeline:</strong></h4>
										<ul>
											<li><strong>Load & inspect data:</strong> Imported multi-statement CSV into pandas. Extracted relevant sections for Balance Sheet, Income Statement, and Cash Flow Statement using <code>loc[]</code>. </li>
											<li><strong>Trend Analysis:</strong> Transposed time-series data <code>(.T)</code> and plotted major components (e.g. assets, equity) using <code>.plot()</code> to visualise financial stability over time. </li>
											<li><strong>Ratio Calculations:</strong> Computed solvency and profitability ratios: Return on Equity (ROE), Return on Assets (ROA), Debt-to-Equity. Built <code>DataFrame</code> to summarise and visualise using grouped bar plots.</li>
											<li><strong>Custom Metrics:</strong> Created Operating Cash Flow to Total Debt ratio from cash flow and balance sheet sections to assess short-term liquidity strength.</li>
											
										</ul>

										<h4>üìä <strong>Code Snippets & Visualisations:</strong></h4>

										<!-- Code Snippet -->

										<pre><code class="language-python"># Importing the libraries
										import tensorflow as tf
										from keras.preprocessing.image import ImageDataGenerator

										# Preprocessing Training set
										train_datagen = ImageDataGenerator(rescale = 1./255,
																		shear_range = 0.2,
																		zoom_range = 0.2,
																		horizontal_flip = True)
										training_set = train_datagen.flow_from_directory('dataset/training_set',
																						target_size = (64, 64),
																						batch_size = 32,
																						class_mode = 'binary')

										# Preprocessing Test set
										test_datagen = ImageDataGenerator(rescale = 1./255)
										test_set = test_datagen.flow_from_directory('dataset/test_set',
																					target_size = (64, 64),
																					batch_size = 32,
																					class_mode = 'binary')

										# Building the CNN

										# Initialising CNN
										cnn = tf.keras.models.Sequential()
										# Step 1 - Convolution
										cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))
										# Step 2 - Pooling
										cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))
										# Step 3 - Adding second layer
										cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))
										cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))
										# Step 4 - Flattening
										cnn.add(tf.keras.layers.Flatten())
										# Step 5 - Full connection
										cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))
										# Step 6 - Flattening
										cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

										# Compiling CNN
										cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

										# Training CNN and evaluating on test set
										cnn.fit(x = training_set, validation_data = test_set, epochs = 25)

										# Making a single prediction
										import numpy as np
										from keras.preprocessing import image
										test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))
										test_image = image.img_to_array(test_image)
										test_image = np.expand_dims(test_image, axis = 0)
										result = cnn.predict(test_image)
										training_set.class_indices
										if result[0][0] == 1:
										prediction = 'dog'
										else:
										prediction = 'cat'

										print(prediction)
										</code></pre>

										
										<!-- Visualisations -->

										<div class="image-gallery">
										<figure>
											<a href="images/Breast_logR.png" data-lightbox="breast-cancer" data-title="Figure 1: Breast_logR">
											<img src="images/Breast_logR.png" alt="Figure 1: Breast_logR">
											</a>
											<figcaption><strong>Figure 1</strong> Logistic Regression</figcaption>
										</figure>

										<figure>
											<a href="images/Breast_svm.png" data-lightbox="breast-cancer" data-title="Figure 2: Breast_svm">
											<img src="images/Breast_svm.png" alt="Figure 2: Breast_svm">
											</a>
											<figcaption><strong>Figure 2</strong> SVM</figcaption>
										</figure>

										<figure>
											<a href="images/Breast_ksvm.png" data-lightbox="breast-cancer" data-title="Figure 3: Breast_ksvm">
											<img src="images/Breast_ksvm.png" alt="Figure 3: Breast_ksvm">
											</a>
											<figcaption><strong>Figure 3</strong> Kernel SVM</figcaption>
										</figure>

										<figure>
											<a href="images/Breast_knn.png" data-lightbox="breast-cancer" data-title="Figure 4: Breast_knn">
											<img src="images/Breast_knn.png" alt="Figure 4: Breast_knn">
											</a>
											<figcaption><strong>Figure 4</strong> K-Nearest Neighbor</figcaption>
										</figure>
										
										<figure>
											<a href="images/Breast_naive.png" data-lightbox="breast-cancer" data-title="Figure 5: Breast_naive">
											<img src="images/Breast_naive.png" alt="Figure 5: Breast_naive">
											</a>
											<figcaption><strong>Figure 5</strong> Na√Øve Bayes</figcaption>
										</figure>

										<figure>
											<a href="images/Breast_randfor.png" data-lightbox="breast-cancer" data-title="Figure 5: Breast_randfor">
											<img src="images/Breast_randfor.png" alt="Figure 2: Breast_randfor">
											</a>
											<figcaption><strong>Figure 6</strong> Random Forest</figcaption>
										</figure>
										
										<figure>
											<a href="images/Breast_decision.png" data-lightbox="breast-cancer" data-title="Figure 5: Breast_decision">
											<img src="images/Breast_decision.png" alt="Figure 2: Breast_decision">
											</a>
											<figcaption><strong>Figure 7</strong> Decision Tree</figcaption>
										</figure>
										</div>

										<h4>üåü <strong>Key Insights:</strong></h4>
										<p>
											The company showed stable asset growth and rising equity, but the Debt-to-Equity ratio increased post-2020, signalling higher leverage risk.
											Operating cash flow was consistently positive, suggesting sufficient liquidity to meet short-term obligations ‚Äî a green flag for operational health.
										</p>

										<h4>üßóüèæ <strong>Challenge Faced:</strong></h4>
										<p>
											Initially, aligning the financial statements by year was inconsistent due to mixed string/index formats across categories. I overcame this by explicitly extracting year-based columns and standardising label references. This made ratio computations and cross-statement comparisons reliable and reproducible.
										</p>
										<p><a href="https://github.com/tendai-codes/your-project" class="github-button" target="_blank">View on GitHub</a></p>
									</section>

									<section class="project-item" data-tags="Python,Deep Learning">
										<h3>üìΩÔ∏è <strong>Natural Language Processing Model to understand resturant reviews </strong></h3>
										<p>
											This project explored publicly available data on top-grossing US films to identify patterns across genres, studios, and release schedules. I used <strong>R</strong> and <strong>ggplot2</strong> for data exploration and visualisation.
										</p>

										<h4>üíª <strong>Tech Stack:</strong></h4>
										<ul>
											<li><strong>Python</strong> for machine learning model development and comparison</li>
											<li><strong>Scikit-learn</strong> for multiple classification algorithms, preprocessing, and evaluation metrics</li>
											<li><strong>Pandas</strong> for dataset loading and initial data exploration</li>
											<li><strong>Matplotlib</strong> for data visualisationsand model performance analysis</li>
											<li><strong>NumPy</strong> for numerical operations and grid generation</li>
										</ul>

										<h4>üß™ <strong>Data Pipeline:</strong></h4>
										<ul>
											<li><strong>Load & inspect data:</strong> Imported multi-statement CSV into pandas. Extracted relevant sections for Balance Sheet, Income Statement, and Cash Flow Statement using <code>loc[]</code>. </li>
											<li><strong>Trend Analysis:</strong> Transposed time-series data <code>(.T)</code> and plotted major components (e.g. assets, equity) using <code>.plot()</code> to visualise financial stability over time. </li>
											<li><strong>Ratio Calculations:</strong> Computed solvency and profitability ratios: Return on Equity (ROE), Return on Assets (ROA), Debt-to-Equity. Built <code>DataFrame</code> to summarise and visualise using grouped bar plots.</li>
											<li><strong>Custom Metrics:</strong> Created Operating Cash Flow to Total Debt ratio from cash flow and balance sheet sections to assess short-term liquidity strength.</li>
											
										</ul>

										<h4>üìä <strong>Code Snippets & Visualisations:</strong></h4>

										<!-- Code Snippet -->

										<pre><code class="language-python"># Importing the libraries
										import re
										import nltk
										nltk.download('stopwords')
										from nltk.corpus import stopwords
										from nltk.stem.porter import PorterStemmer
										corpus = []
										for i in range(0, 1000):
										review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])
										review = review.lower()
										review = review.split()
										ps = PorterStemmer()
										all_stopwords = stopwords.words('english')
										all_stopwords.remove('not')
										review = [ps.stem(word) for word in review if not word in set(all_stopwords)]
										review = ' '.join(review)
										corpus.append(review)

										print(corpus) (Table 1)

										# Creating the bag of words model
										from sklearn.feature_extraction.text import CountVectorizer
										cv = CountVectorizer(max_features = 1500)
										X = cv.fit_transform(corpus).toarray()
										y = dataset.iloc[:, -1].values

										# Splitting the dataset into the Training set and Test set

										from sklearn.model_selection import train_test_split
										X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)

										# Training the Naive Bayes model on the Training set
										from sklearn.naive_bayes import GaussianNB
										classifier = GaussianNB()
										classifier.fit(X_train, y_train)

										# Predicting the Test set results (Table 2)
										y_pred = classifier.predict(X_test)
										print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

										# Making Confusion Matrix (Table 3)
										from sklearn.metrics import confusion_matrix, accuracy_score
										cm = confusion_matrix(y_test, y_pred)
										print(cm)
										accuracy_score(y_test, y_pred)
										</code></pre>

										
										<!-- Visualisations -->

										<div class="image-gallery">
										<figure>
											<a href="images/Breast_logR.png" data-lightbox="breast-cancer" data-title="Figure 1: Breast_logR">
											<img src="images/Breast_logR.png" alt="Figure 1: Breast_logR">
											</a>
											<figcaption><strong>Figure 1</strong> Logistic Regression</figcaption>
										</figure>

										<figure>
											<a href="images/Breast_svm.png" data-lightbox="breast-cancer" data-title="Figure 2: Breast_svm">
											<img src="images/Breast_svm.png" alt="Figure 2: Breast_svm">
											</a>
											<figcaption><strong>Figure 2</strong> SVM</figcaption>
										</figure>

										<figure>
											<a href="images/Breast_ksvm.png" data-lightbox="breast-cancer" data-title="Figure 3: Breast_ksvm">
											<img src="images/Breast_ksvm.png" alt="Figure 3: Breast_ksvm">
											</a>
											<figcaption><strong>Figure 3</strong> Kernel SVM</figcaption>
										</figure>

										<figure>
											<a href="images/Breast_knn.png" data-lightbox="breast-cancer" data-title="Figure 4: Breast_knn">
											<img src="images/Breast_knn.png" alt="Figure 4: Breast_knn">
											</a>
											<figcaption><strong>Figure 4</strong> K-Nearest Neighbor</figcaption>
										</figure>
										
										<figure>
											<a href="images/Breast_naive.png" data-lightbox="breast-cancer" data-title="Figure 5: Breast_naive">
											<img src="images/Breast_naive.png" alt="Figure 5: Breast_naive">
											</a>
											<figcaption><strong>Figure 5</strong> Na√Øve Bayes</figcaption>
										</figure>

										<figure>
											<a href="images/Breast_randfor.png" data-lightbox="breast-cancer" data-title="Figure 5: Breast_randfor">
											<img src="images/Breast_randfor.png" alt="Figure 2: Breast_randfor">
											</a>
											<figcaption><strong>Figure 6</strong> Random Forest</figcaption>
										</figure>
										
										<figure>
											<a href="images/Breast_decision.png" data-lightbox="breast-cancer" data-title="Figure 5: Breast_decision">
											<img src="images/Breast_decision.png" alt="Figure 2: Breast_decision">
											</a>
											<figcaption><strong>Figure 7</strong> Decision Tree</figcaption>
										</figure>
										</div>

										<h4>üåü <strong>Key Insights:</strong></h4>
										<p>
											The company showed stable asset growth and rising equity, but the Debt-to-Equity ratio increased post-2020, signalling higher leverage risk.
											Operating cash flow was consistently positive, suggesting sufficient liquidity to meet short-term obligations ‚Äî a green flag for operational health.
										</p>

										<h4>üßóüèæ <strong>Challenge Faced:</strong></h4>
										<p>
											Initially, aligning the financial statements by year was inconsistent due to mixed string/index formats across categories. I overcame this by explicitly extracting year-based columns and standardising label references. This made ratio computations and cross-statement comparisons reliable and reproducible.
										</p>
										<p><a href="https://github.com/tendai-codes/your-project" class="github-button" target="_blank">View on GitHub</a></p>
									</section>

								</div>
							</article>

							

						<!-- About Me -->
							<article id="about">
								<h2 class="major">About Me</h2>
								<span class="bio-container"><img src="images/bio.jpg" alt="" style="width: 70%; max-width: 550px; margin-left: 90px; border-radius: 4px; "/>
								</span>
								<p>I am an aspiring data scientist and educator focused on curriculum innovation, AI-driven assessment, and machine learning for real-world educational impact. I attained my BSc in Liberal Arts & Sciences from Maastricht University, where I learned to blend critical thinking with interdisciplinary research‚Äîdeveloping a mindset for tackling complex, real-world challenges.</p>
								<p>At the University of Manchester, I took a deep dive into molecular biology research through an MRes in Tissue Engineering for Regenerative Medicine. There, I developed a gene therapy model for a genetic muscle-wasting disease, which acted as a proof-of-principle to support early stage clinical trials. During this time I strengthened my hypothesis-driven probelm solving approach and my ability to leverage data to tell a stories at the intersection of biomedical engineering, and data analysis.</p>
								<p>As a former science coordinator in the UAE public education system, I designed national exams, and later began integrating generative AI tools into the classroom. These experiences sparked my current focus of understanding  data to  help build AI systems that empower with how people learn‚Äîethically, adaptively, and with lasting impact.</p>
							</article>

						<!-- Contact -->
							<article id="contact">
								<h2 class="major">Contact</h2>
								<form method="post" action="#">
									<div class="fields">
										<div class="field half">
											<label for="name">Name</label>
											<input type="text" name="name" id="name" />
										</div>
										<div class="field half">
											<label for="email">Email</label>
											<input type="text" name="email" id="email" />
										</div>
										<div class="field">
											<label for="message">Message</label>
											<textarea name="message" id="message" rows="4"></textarea>
										</div>
									</div>
									<ul class="actions">
										<li><input type="submit" value="Send Message" class="primary" /></li>
										<li><input type="reset" value="Reset" /></li>
									</ul>
								</form>
								<ul class="icons">
									<li><a href="https://www.linkedin.com/in/tendai-sibanda" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
									<li><a href="https://github.com/tendai-codes" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
								</ul>
							</article>

						<!-- Elements -->
							<article id="elements">
								<h2 class="major">Elements</h2>

								<section>
									<h3 class="major">Text</h3>
									<p>This is <b>bold</b> and this is <strong>strong</strong>. This is <i>italic</i> and this is <em>emphasized</em>.
									This is <sup>superscript</sup> text and this is <sub>subscript</sub> text.
									This is <u>underlined</u> and this is code: <code>for (;;) { ... }</code>. Finally, <a href="#">this is a link</a>.</p>
									<hr />
									<h2>Heading Level 2</h2>
									<h3>Heading Level 3</h3>
									<h4>Heading Level 4</h4>
									<h5>Heading Level 5</h5>
									<h6>Heading Level 6</h6>
									<hr />
									<h4>Blockquote</h4>
									<blockquote>Fringilla nisl. Donec accumsan interdum nisi, quis tincidunt felis sagittis eget tempus euismod. Vestibulum ante ipsum primis in faucibus vestibulum. Blandit adipiscing eu felis iaculis volutpat ac adipiscing accumsan faucibus. Vestibulum ante ipsum primis in faucibus lorem ipsum dolor sit amet nullam adipiscing eu felis.</blockquote>
									<h4>Preformatted</h4>
									<pre><code>i = 0;

while (!deck.isInOrder()) {
    print 'Iteration ' + i;
    deck.shuffle();
    i++;
}

print 'It took ' + i + ' iterations to sort the deck.';</code></pre>
								</section>

								<section>
									<h3 class="major">Lists</h3>

									<h4>Unordered</h4>
									<ul>
										<li>Dolor pulvinar etiam.</li>
										<li>Sagittis adipiscing.</li>
										<li>Felis enim feugiat.</li>
									</ul>

									<h4>Alternate</h4>
									<ul class="alt">
										<li>Dolor pulvinar etiam.</li>
										<li>Sagittis adipiscing.</li>
										<li>Felis enim feugiat.</li>
									</ul>

									<h4>Ordered</h4>
									<ol>
										<li>Dolor pulvinar etiam.</li>
										<li>Etiam vel felis viverra.</li>
										<li>Felis enim feugiat.</li>
										<li>Dolor pulvinar etiam.</li>
										<li>Etiam vel felis lorem.</li>
										<li>Felis enim et feugiat.</li>
									</ol>
									<h4>Icons</h4>
									<ul class="icons">
										<li><a href="https://www.linkedin.com/in/tendai-sibanda" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
									<li><a href="https://github.com/tendai-codes" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
								</ul>

									<h4>Actions</h4>
									<ul class="actions">
										<li><a href="#" class="button primary">Default</a></li>
										<li><a href="#" class="button">Default</a></li>
									</ul>
									<ul class="actions stacked">
										<li><a href="#" class="button primary">Default</a></li>
										<li><a href="#" class="button">Default</a></li>
									</ul>
								</section>

								<section>
									<h3 class="major">Table</h3>
									<h4>Default</h4>
									<div class="table-wrapper">
										<table>
											<thead>
												<tr>
													<th>Name</th>
													<th>Description</th>
													<th>Price</th>
												</tr>
											</thead>
											<tbody>
												<tr>
													<td>Item One</td>
													<td>Ante turpis integer aliquet porttitor.</td>
													<td>29.99</td>
												</tr>
												<tr>
													<td>Item Two</td>
													<td>Vis ac commodo adipiscing arcu aliquet.</td>
													<td>19.99</td>
												</tr>
												<tr>
													<td>Item Three</td>
													<td> Morbi faucibus arcu accumsan lorem.</td>
													<td>29.99</td>
												</tr>
												<tr>
													<td>Item Four</td>
													<td>Vitae integer tempus condimentum.</td>
													<td>19.99</td>
												</tr>
												<tr>
													<td>Item Five</td>
													<td>Ante turpis integer aliquet porttitor.</td>
													<td>29.99</td>
												</tr>
											</tbody>
											<tfoot>
												<tr>
													<td colspan="2"></td>
													<td>100.00</td>
												</tr>
											</tfoot>
										</table>
									</div>

									<h4>Alternate</h4>
									<div class="table-wrapper">
										<table class="alt">
											<thead>
												<tr>
													<th>Name</th>
													<th>Description</th>
													<th>Price</th>
												</tr>
											</thead>
											<tbody>
												<tr>
													<td>Item One</td>
													<td>Ante turpis integer aliquet porttitor.</td>
													<td>29.99</td>
												</tr>
												<tr>
													<td>Item Two</td>
													<td>Vis ac commodo adipiscing arcu aliquet.</td>
													<td>19.99</td>
												</tr>
												<tr>
													<td>Item Three</td>
													<td> Morbi faucibus arcu accumsan lorem.</td>
													<td>29.99</td>
												</tr>
												<tr>
													<td>Item Four</td>
													<td>Vitae integer tempus condimentum.</td>
													<td>19.99</td>
												</tr>
												<tr>
													<td>Item Five</td>
													<td>Ante turpis integer aliquet porttitor.</td>
													<td>29.99</td>
												</tr>
											</tbody>
											<tfoot>
												<tr>
													<td colspan="2"></td>
													<td>100.00</td>
												</tr>
											</tfoot>
										</table>
									</div>
								</section>

								<section>
									<h3 class="major">Buttons</h3>
									<ul class="actions">
										<li><a href="#" class="button primary">Primary</a></li>
										<li><a href="#" class="button">Default</a></li>
									</ul>
									<ul class="actions">
										<li><a href="#" class="button">Default</a></li>
										<li><a href="#" class="button small">Small</a></li>
									</ul>
									<ul class="actions">
										<li><a href="#" class="button primary icon solid fa-download">Icon</a></li>
										<li><a href="#" class="button icon solid fa-download">Icon</a></li>
									</ul>
									<ul class="actions">
										<li><span class="button primary disabled">Disabled</span></li>
										<li><span class="button disabled">Disabled</span></li>
									</ul>
								</section>

								<section>
									<h3 class="major">Form</h3>
									<form method="post" action="#">
										<div class="fields">
											<div class="field half">
												<label for="demo-name">Name</label>
												<input type="text" name="demo-name" id="demo-name" value="" placeholder="Jane Doe" />
											</div>
											<div class="field half">
												<label for="demo-email">Email</label>
												<input type="email" name="demo-email" id="demo-email" value="" placeholder="jane@untitled.tld" />
											</div>
											<div class="field">
												<label for="demo-category">Category</label>
												<select name="demo-category" id="demo-category">
													<option value="">-</option>
													<option value="1">Manufacturing</option>
													<option value="1">Shipping</option>
													<option value="1">Administration</option>
													<option value="1">Human Resources</option>
												</select>
											</div>
											<div class="field half">
												<input type="radio" id="demo-priority-low" name="demo-priority" checked>
												<label for="demo-priority-low">Low</label>
											</div>
											<div class="field half">
												<input type="radio" id="demo-priority-high" name="demo-priority">
												<label for="demo-priority-high">High</label>
											</div>
											<div class="field half">
												<input type="checkbox" id="demo-copy" name="demo-copy">
												<label for="demo-copy">Email me a copy</label>
											</div>
											<div class="field half">
												<input type="checkbox" id="demo-human" name="demo-human" checked>
												<label for="demo-human">Not a robot</label>
											</div>
											<div class="field">
												<label for="demo-message">Message</label>
												<textarea name="demo-message" id="demo-message" placeholder="Enter your message" rows="6"></textarea>
											</div>
										</div>
										<ul class="actions">
											<li><input type="submit" value="Send Message" class="primary" /></li>
											<li><input type="reset" value="Reset" /></li>
										</ul>
									</form>
								</section>

							</article>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<p class="copyright">&copy; Untitled. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>

			</div>

		<!-- BG -->
			<div id="bg"></div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

		<script>
        document.addEventListener("DOMContentLoaded", () => {
			const buttons = document.querySelectorAll(".filter-button");
			const projects = document.querySelectorAll(".project-item");

			//  Hide all projects by default
			projects.forEach(project => {
				project.style.display = "none";
			});

			// Add event listener to all buttons
			buttons.forEach(button => {
				button.addEventListener("click", () => {
					// Get selected tag from button
					const tag = button.getAttribute("data-tag");
					activeTag = tag;

					// Highlight the active button
					buttons.forEach(btn => btn.classList.remove("active"));
					button.classList.add("active");

					// Filter projects
					projects.forEach(project => {
						const tags = project.getAttribute("data-tags").split(",").map(tag => tag.trim());
						if (tag === "all" || tags.includes(tag)) {
							project.style.display = "block";
						} else {
							project.style.display = "none";
						}
					});
				});
			});
		});
	
    	</script>

		<!-- Highlight.js core script -->
		<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

		<!-- Optional: Load specific languages -->
		<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/r.min.js"></script>

		<!-- Initialise Highlight.js -->
		<script>hljs.highlightAll();</script>

		<!-- Add jquery -->
		<script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
	
		<!-- Lightbox JS -->
		<script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.4/js/lightbox.min.js"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.4/js/lightbox.group.min.js"></script>

		
		<script>
    // Initialize Lightbox with custom settings
    lightbox.option({
        'wrapAround': true,
        'maxWidth': '60%',
        'maxHeight': '90vh',
        'positionFromTop': 50,
        'positionFromLeft': 50,
        'enableKeyboardInput': true,
        'showImageNumberLabel': false
		'resizeImage': true,
		'wrapAround': false
    });

    // Add custom functionality to allow scrolling through images while zoomed in
    $(document).ready(function() {
        lightbox.addEventListener('afterOpen', function() {
            // Add a "Next" and "Previous" button inside the overlay
            var controls = $('<div>').addClass('image-controls')
                .append('<button class="prev">Previous</button>')
                .append('<button class="next">Next</button>');
            
            $('.lb-overlay').prepend(controls);

            // Set up click handlers for navigation
            $('.image-controls .prev').click(function() {
                lightbox.close();
                lightbox.gotoPrev();
                lightbox.open();
            });

            $('.image-controls .next').click(function() {
                lightbox.close();
                lightbox.gotoNext();
                lightbox.open();
            });
        });
    });
</script>
	</body>
</html>
